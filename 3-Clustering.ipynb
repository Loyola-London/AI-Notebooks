{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591593c8-b5ff-451a-9837-469ba56bd3b0",
   "metadata": {},
   "source": [
    "# Module 03 - Clustering\n",
    "## Sample Purification\n",
    "Sample purification is a critical preprocessing step in the machine learning process that focuses on improving data quality to ensure more reliable and accurate model predictions. This process involves systematically examining and cleaning the dataset to address various data quality issues that could potentially skew model results or lead to misleading conclusions. The two primary concerns in sample purification are the presence of outliers, which can disproportionately influence model parameters, and missing data, which can introduce bias and reduce the effective sample size available for analysis.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://thislondonhouse.com/Images/outlier.png\" style=\"height: 150px;\">\n",
    "    <img src=\"https://thislondonhouse.com/Images/missing_data.png\" style=\"height: 150px;\"></p>\n",
    "\n",
    "### Outlier Detection\n",
    "Handling outliers requires a careful balance between maintaining data integrity while preventing extreme values from unduly influencing model performance. Outliers are data points that deviate significantly from the rest of the dataset. They can skew results and lead to inaccurate model predictions. To handle outliers, we must first identify outliers and then apply methods to mitigate their impact on the model. To identify outliers, we can use statistical techniques like z-scores or IQR (Interquartile Range) to detect values that are in the extreme with regard to the other data points. Once identified, outliers can be handled through various approaches: they may be removed if they're determined to be errors, transformed to reduce their impact, or analyzed separately to understand if they represent meaningful but rare patterns in the data. It's crucial to document any outlier treatment decisions and their justification, as these choices can significantly impact the model's performance.\n",
    "\n",
    "### Missing Data\n",
    "Missing data is a common issue in real-world datasets and can compromise the quality of machine learning models. Often, records with missing values are discarded as there is little theoretical justification for generating synthetic data to fill missing values. However, there are other strategies to handle missing data, including imputation, which involves filling in missing values using statistical methods like mean, median, or mode. Another approach is to use advanced techniques like K-Nearest Neighbors (KNN) imputation, where the missing value is estimated based on the values of the nearest neighbors. In some cases, if the proportion of missing data is substantial, it may be better to discard the affected features or samples altogether.\n",
    "\n",
    "## Feature Engineering\n",
    "Feature engineering is a crucial aspect of the machine learning pipeline that involves creating and transforming features to improve model performance. This process requires both domain expertise and technical knowledge to identify and extract meaningful patterns from raw data, ultimately presenting the information in a way that machine learning algorithms can better understand and utilize. Feature engineering can significantly impact model performance, often proving more influential than the choice of algorithm itself, as it helps expose the underlying patterns and relationships that might otherwise remain hidden in the raw data.  \n",
    "\n",
    "A common feature creation is a common and important feature engineering task. Feature creation is the process of generating new features based on existing data. This can include combining multiple features into interaction terms, creating polynomial features, or deriving new features using domain knowledge. For example, in a dataset containing date information, one might create new features like \"day of the week\" or \"month of the year\" to capture temporal patterns. Another example is generating industry specific ratios such as \"income-to-expenditure\", that provide additional insight into financial behavior. Feature creation helps in capturing underlying patterns and relationships that may not be immediately apparent in the raw data.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://thislondonhouse.com/Images/ratios.png\" style=\"height: 150px;\"></p>\n",
    "\n",
    "## Dimensionality Reduction\n",
    "Dimensionality reduction is the process of reducing the number of features (or dimensions) in a dataset while preserving as much of the relevant information as possible. High-dimensional datasets can be challenging to work with due to the \"curse of dimensionality,\" which refers to various problems that arise when analyzing and organizing data in high-dimensional spaces. These issues include increased computational complexity, risk of overfitting, and difficulties in visualizing and interpreting the data. By reducing the dimensionality, we can simplify the dataset, making it easier to analyze and model, while also potentially improving the performance of machine learning algorithms by removing redundant and irrelevant features.\n",
    "\n",
    "Dimensionality reduction can be achieved by a technique known as feature extraction. Feature extraction involves creating new features by combining or transforming the original ones. Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are popular methods for feature extraction. PCA reduces dimensionality by projecting data onto a lower-dimensional space defined by principal components, which capture the maximum variance in the data. LDA, used primarily for classification tasks, finds the linear combinations of features that best separate different classes. Both techniques help retain the most important information in the data while reducing the number of dimensions, leading to more efficient and effective machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f15b5dd-cab9-46de-b0e7-e3accad72e40",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Clustering is a fundamental unsupervised learning technique that aims to discover natural groupings or patterns within data without the guidance of predefined labels or outcomes. Clustering algorithms seek to group a set of data points into clusters, where data points in the same cluster are more similar to each other than to those in other clusters. By identifying groups of similar objects and separating dissimilar ones, clustering helps reveal hidden structures and relationships in complex datasets that might not be immediately apparent. This approach is particularly valuable across diverse fields: in marketing, it helps segment customers based on behavior patterns; in biology, it can group genes with similar expression patterns; in document analysis, it organizes text by topic; and in anomaly detection, it identifies unusual patterns that deviate from normal clusters. The power of clustering lies in its ability to automatically discover these natural groupings, making it an essential tool for exploratory data analysis, pattern recognition, and data-driven decision making.\n",
    "\n",
    "The process of clustering involves several key components and decisions that determine how groups are formed and what constitutes similarity between data points. At its core, clustering algorithms rely on distance or similarity metrics (such as Euclidean distance, cosine similarity, or Manhattan distance) to measure how close or similar data points are to each other. Different clustering algorithms approach group formation in distinct ways: partitioning methods like K-means divide data into a predetermined number of clusters, hierarchical clustering builds a tree-like structure of nested clusters, density-based methods like DBSCAN identify clusters based on areas of high data point density, and probabilistic approaches like Gaussian Mixture Models use statistical distributions to model cluster membership. The choice of algorithm depends on factors such as the shape and distribution of the expected clusters, the scale of the data, the presence of noise, and whether the number of clusters needs to be specified in advance. Through these methods, unsupervised clustering helps reveal the natural grouping within data, providing insights that guide further analysis and decision-making.\n",
    "\n",
    "### Clustering Algorithms\n",
    "#### Partitioning Methods\n",
    "Partitioning methods are clustering techniques that divide a dataset into a predefined number of clusters. The most common partitioning method is K-Means clustering, which starts by initializing K centroids randomly. Each data point is then assigned to the nearest centroid, and the centroids are updated based on the mean of the assigned points. This process is repeated iteratively until the centroids stabilize and the clusters become well-defined. Another example is the K-Medoids algorithm, which is similar to K-Means but uses actual data points as centroids (medoids) instead of the mean. Partitioning methods are efficient for large datasets and provide a straightforward way to categorize data. However, they require the number of clusters to be specified in advance and may struggle with non-spherical cluster shapes.\n",
    "\n",
    "#### Hierarchical Methods\n",
    "Hierarchical clustering methods build a tree-like structure of nested clusters through successive merging or splitting based on similarity. There are two main types: agglomerative (bottom-up) and divisive (top-down). Agglomerative clustering starts with each data point as its own cluster and iteratively merges the closest pairs of clusters until only one cluster remains. Divisive clustering, on the other hand, begins with a single cluster containing all data points and successively splits it into smaller clusters. Algorithms such as Agglomerative Hierarchical Clustering (AHC) and DIANA (Divisive Analysis) are commonly used in hierarchical methods. These methods do not require the number of clusters to be specified beforehand and are useful for identifying clusters at different levels of granularity. However, they can be computationally intensive, especially for large datasets.\n",
    "\n",
    "#### Density-Based Methods\n",
    "Density-based clustering methods group data points based on regions of high density, allowing them to discover clusters of arbitrary shapes. A popular density-based method is DBSCAN (Density-Based Spatial Clustering of Applications with Noise), which defines clusters as areas with a high density of points separated by areas of lower density. DBSCAN starts by selecting a random point and identifying its neighborhood within a specified distance (epsilon). If the neighborhood contains at least a minimum number of points (minPts), the point is considered a core point, and a cluster is formed. The process continues by expanding the cluster with neighboring core points and their neighborhoods. Another density-based method is OPTICS (Ordering Points To Identify the Clustering Structure), which is similar to DBSCAN but can handle varying densities within the same dataset. Density-based methods are effective in handling noise and outliers and can identify clusters of various shapes without requiring the number of clusters to be specified in advance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63d162-30dc-415c-967b-4c6cbbb1a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a06a0-d4e8-4678-9575-803d8f8a3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster performance reports statistics and summaries of cluster performance and feature distributions.\n",
    "def plot_descriptives(df):\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Plot setup\n",
    "    num_plots = len(numeric_cols) + len(categorical_cols)\n",
    "    cols = 3\n",
    "    rows = int(np.ceil(num_plots / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Numeric columns: histograms\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        sns.histplot(df[col], bins=10, ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Categorical columns: bar charts\n",
    "    for j, col in enumerate(categorical_cols, start=len(numeric_cols)):\n",
    "        sns.countplot(x=df[col], ax=axes[j])\n",
    "        axes[j].set_title(f'Counts of {col}')\n",
    "        axes[j].set_xlabel(col)\n",
    "        axes[j].set_ylabel('Count')\n",
    "        axes[j].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Remove unused axes\n",
    "    for k in range(num_plots, len(axes)):\n",
    "        fig.delaxes(axes[k])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def cluster_preformance(X, labels):\n",
    "    # Calculate evaluation metrics\n",
    "    silhouette_avg = metrics.silhouette_score(X, labels)\n",
    "    ch_score = metrics.calinski_harabasz_score(X, labels)\n",
    "    db_score = metrics.davies_bouldin_score(X, labels)\n",
    "\n",
    "    print(f'Silhouette Score: {silhouette_avg}')\n",
    "    print(f'Calinski-Harabasz Score: {ch_score}')\n",
    "    print(f'Davies-Bouldin Score: {db_score}')\n",
    "\n",
    "    df = X.copy()\n",
    "    data_cols = df.columns\n",
    "    df['cluster'] = labels\n",
    "\n",
    "    # Create boxen plots for each feature in a single figure\n",
    "    plt.figure(figsize=(10, int(len(data_cols)/2)))\n",
    "    plt.subplot(int(np.ceil((1+len(data_cols))/3)), 3, 1)\n",
    "    ax = sns.countplot(x='cluster', hue='cluster', legend=False, data=df, palette=\"Set1\")\n",
    "\n",
    "    # Plot each feature separately\n",
    "    for i, feature in enumerate(data_cols):\n",
    "        plt.subplot(int(np.ceil((1+len(data_cols))/3)), 3, i+2)\n",
    "        ax = sns.boxenplot(x=feature, hue='cluster', data=df, palette=\"Set1\")\n",
    "        plt.legend(loc=\"upper right\", title='Cluster', fontsize='xx-small', title_fontsize='small')\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Find the optimal number of clusters for sklearn clustering algorithms. These algorithms either require an apriori selection of clusters or\n",
    "# require an epsilon factor that greatly affects how clusters are estimated. This function iterates through a various values of n_clusters or eps\n",
    "# and provides the hyperparameters with the best performing results.\n",
    "def find_optimal_clusters(clustering_pipeline, X):\n",
    "    cluster_metrics = []\n",
    "    i_range = range(2,16)\n",
    "\n",
    "    for i in i_range:\n",
    "        if isinstance(clustering_pipeline['model'], KMeans) or isinstance(clustering_pipeline['model'], AgglomerativeClustering):\n",
    "            clustering_pipeline['model'].n_clusters = i\n",
    "            metric = 'n_clusters'\n",
    "            value = clustering_pipeline['model'].n_clusters\n",
    "        elif isinstance(clustering_pipeline['model'], DBSCAN):\n",
    "            clustering_pipeline['model'].eps = i / 4\n",
    "            clustering_pipeline['model'].min_samples = len(X.columns) * 2\n",
    "            metric = 'eps'\n",
    "            value = clustering_pipeline['model'].eps\n",
    "\n",
    "\n",
    "        # Predict cluster labels\n",
    "        labels = pipeline.fit_predict(X)\n",
    "        # print(metric, value)\n",
    "\n",
    "        if len(np.unique(labels)) > 1:\n",
    "            cluster_metrics.append({'i': value, \n",
    "                                    'j': 1,\n",
    "                                    'silhouette_score': metrics.silhouette_score(X, labels),\n",
    "                                    'calinski_harabasz_score': metrics.calinski_harabasz_score(X, labels),\n",
    "                                    'davies_bouldin_score': metrics.davies_bouldin_score(X, labels),\n",
    "                                   })\n",
    "\n",
    "    cluster_metrics_df = pd.DataFrame(cluster_metrics)\n",
    "\n",
    "    fig, axs = plt.subplots(1,3, figsize=(12, 4))\n",
    "\n",
    "    # Plot the first subplot\n",
    "    for key, grp in cluster_metrics_df.groupby(['j']):\n",
    "        axs[0].plot(grp['i'], grp['silhouette_score'], label=key, marker='o')\n",
    "        axs[0].set_title('Silhouette Score')\n",
    "\n",
    "    # Plot the second subplot\n",
    "    for key, grp in cluster_metrics_df.groupby(['j']):\n",
    "        axs[1].plot(grp['i'], grp['calinski_harabasz_score'], label=key, marker='o')\n",
    "        axs[1].set_title('Calinski-Harabasz Index')\n",
    "\n",
    "    # Plot the third subplot\n",
    "    for key, grp in cluster_metrics_df.groupby(['j']):\n",
    "        axs[2].plot(grp['i'], grp['davies_bouldin_score'], label=key, marker='o')\n",
    "        axs[2].set_title('Davies-Bouldin Index')\n",
    "\n",
    "    # Adjust the layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return {metric: {'silhouette': cluster_metrics_df.loc[cluster_metrics_df['silhouette_score'].idxmax(), 'i'],\n",
    "           'calinski_harabasz': cluster_metrics_df.loc[cluster_metrics_df['calinski_harabasz_score'].idxmax(), 'i'],\n",
    "           'davies_bouldin': cluster_metrics_df.loc[cluster_metrics_df['davies_bouldin_score'].idxmin(), 'i']}}\n",
    "\n",
    "# Find the optimal number of feature dimensions. Used to reduce the number of features and limit the effects of overfitting.\n",
    "def find_optimal_dimensions(X):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "    # Fit PCA model\n",
    "    pca = PCA()\n",
    "    pca.fit(X_scaled)\n",
    "\n",
    "    # Calculate explained variance\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    # Calculate cumulative explained variance\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "    fig, axs = plt.subplots(2,1, figsize=(6, 8))\n",
    "\n",
    "    # Create scree plot\n",
    "    axs[0].plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "    axs[0].set_title('Scree Plot')\n",
    "\n",
    "    # Create cumulative explained variance plot\n",
    "    axs[1].plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "    axs[1].set_xlabel('Principal Component')\n",
    "    axs[1].set_title('Cumulative Explained Variance Plot')\n",
    "    axs[1].axhline(y=0.9, color='r', linestyle='-')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a9b80-738a-4ee2-ac85-e3d3da20ef44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clustering Exercise 1\n",
    "### Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b10160f-2319-4f74-b717-259e4f16a4bc",
   "metadata": {},
   "source": [
    "Increasingly, shoppers are anonymous. Whereas decades ago, smaller retailers may have a decent understanding of who their customers are and what their needs may be, the rise of online shopping and the supplantating of smaller retailers by national chains ensures that business 'know' little about their custoemrs. Therefore, it is important that retailers mine customer data to better understand the types of customers that frequent their shops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3494727f-990b-4bff-b691-7bebd4788c84",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "We will be loading data from a customer dataset. More information is available [here](https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign).  \n",
    "\n",
    "Data are orgnized in tabular format with each record representing an individual customer. There is not target variable. The data contain the following columns:  \n",
    "| Variable Name | Role | Type | Description |\n",
    "| ------------- | ---- | ---- | ----------- |\n",
    "| ID | Feature | Integer | Customer ID | \n",
    "| Year_Birth | Feature | Integer | 4-digit year when customer was born | \n",
    "| Education | Feature | Categorical | Education Level | \n",
    "| Marital_Status | Feature | Categorical | Marital Status | \n",
    "| Income | Feature | Integer | Annual Income | \n",
    "| Kidhome | Feature | Integer | Number of younger children at home | \n",
    "| Teenhome | Feature | Integer | Number of older children (teenagers) at home | \n",
    "| Dt_Customer_Start | Feature | Date of first purchase |  | \n",
    "| Recency | Feature | Integer | Days since most recent purchase | \n",
    "| MntWines | Feature | Integer | Amoung spent on Wines | \n",
    "| MntFruits | Feature | Integer | Amoung spent on Fruits | \n",
    "| MntMeatProducts | Feature | Integer | Amoung spent on Meat | \n",
    "| MntFishProducts |Feature  | Integer | Amoung spent on Fish | \n",
    "| MntSweetProducts | Feature | Integer | Amoung spent on Sweets | \n",
    "| MntGoldProds | Feature | Integer | Amoung spent on Gold | \n",
    "| NumDealsPurchases | Feature | Integer | Number of purchases made with promotions | \n",
    "| NumWebPurchases | Feature | Integer | Number of purchases through website | \n",
    "| NumCatalogPurchases | Feature | Integer | Number of purchases through catalog | \n",
    "| NumStorePurchases | Feature | Integer | Number of in-store purchases | \n",
    "| NumWebVisitsMonth | Feature | Integer | Number of visits to website per month | \n",
    "| AcceptedCmp3 | Feature | Binary | Customer made purchase during campaign 3 | \n",
    "| AcceptedCmp4 | Feature | Binary | Customer made purchase during campaign 4 | \n",
    "| AcceptedCmp5 | Feature | Binary | Customer made purchase during campaign 5 | \n",
    "| AcceptedCmp1 | Feature | Binary | Customer made purchase during campaign 1 | \n",
    "| AcceptedCmp2 | Feature | Binary | Customer made purchase during campaign 2 | \n",
    "| Complain | Feature | Binary | Customer has logged a complaint | \n",
    "\n",
    "\n",
    "The following line will load the data as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2893f-0a1a-49e0-aa75-b7574a37c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pd.read_csv('data/customer_campaign.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcb290-bf87-40f2-945a-93263b95193e",
   "metadata": {},
   "source": [
    "### Data Profiling\n",
    "Profile the data and prepare it for analysis. \n",
    "\n",
    "The following line provides an overview of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e562c-6d34-4497-8a5c-a4f69f995de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f0c6b-cfbb-40e2-a033-a8cb34e3d634",
   "metadata": {},
   "source": [
    "This peeks at the first rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7c005-a34f-468a-8b0d-ca0b95d64c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd82aec-e1fe-4b87-a10e-349d12358735",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "We have two date columns. It is not obvious how dates should be handled and it often depends on the machine learning task. In our case, we are going to use the dates to get a better understanding of our customer. First we will convert the start date into a date-type filed so that Python has means of manipulating the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a75781-74ec-4a36-9946-08469fbca9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df['Dt_Customer_Start'] = pd.to_datetime(customer_df['Dt_Customer_Start'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d7a71-478d-4b10-b771-8f07923a8f9b",
   "metadata": {},
   "source": [
    "Next, we will use the date start and the birth year fields to calculate the age of customer at the time of first purchase. Depending on the business, this may be a helpful field for understanding customers. If the business has been in business for more 20 or more years, this field will be less helpful because the overall distribution of ages will begin to look more like the population at large. However, if the business is realtively new, it could help us understand whether the business appeals to younger or older individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f46d47-3cac-4bf7-9bcb-15ef66ddea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df['age'] = customer_df.apply(lambda row: row['Dt_Customer_Start'].year - row['Year_Birth'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13593d9a-c21d-481c-bf54-8fbedae67dce",
   "metadata": {},
   "source": [
    "Next, we are going to create a field which calculates the total number of children in the house. While some retailers may need to know the proportion of older/younger children, two features for measuring children is likely to introduce noise into the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053af4a1-8490-4d3d-a098-328c9dcb9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df['total_children'] = customer_df['Kidhome'] + customer_df['Teenhome']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102da58-ae5e-4eda-bb56-78e44eecce56",
   "metadata": {},
   "source": [
    "Now, we want to create some new fields based on purchase behavior. We will create fiels which calculate the total spent across all categories, total number of purchases across all venues and the total participation in promotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65134363-a4ee-4cb9-8de5-8f7871331841",
   "metadata": {},
   "outputs": [],
   "source": [
    "spent_cols = ['MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds']\n",
    "purchase_cols = ['NumWebPurchases','NumCatalogPurchases','NumStorePurchases']\n",
    "promotion_cols = ['AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','AcceptedCmp1','AcceptedCmp2']\n",
    "\n",
    "customer_df['total_spent'] = customer_df[spent_cols].sum(axis=1)\n",
    "customer_df['total_purchases'] = customer_df[purchase_cols].sum(axis=1)\n",
    "customer_df['total_promotions'] = customer_df[promotion_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ee1ba-86e4-4411-8832-01f6ef442c3d",
   "metadata": {},
   "source": [
    "We will use those values to calculate rates. First, we will calculate average ticket prices; on average, how much did the customer spend each time they made a purchase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27aaf9-83a9-44e2-bb08-4254e137e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df['avg_spent'] = customer_df['total_spent'] / customer_df['total_purchases']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fdec36-f219-4ef4-b5e4-f9ad84cca7b4",
   "metadata": {},
   "source": [
    "Second, we will calculate the proprotion of purchases made during promotions. Of the purchases made, what percent were induced by promotions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8c0423-b7d9-4f1c-b28a-b0568a8ee4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df['promotion_pct'] = customer_df['NumDealsPurchases'] / customer_df['total_purchases']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca6d5a-b4f6-42c6-95e6-46138dd1f638",
   "metadata": {},
   "source": [
    "Third, what is the response rate for promotions? Are customers taking advantage of all promotions or are they indifferent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b9a37e-0784-41b1-bc50-802b4f656f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df['promotion_response_rt'] = customer_df['total_promotions'] / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3839fd3-df3f-48e1-b7c9-e6c95a178452",
   "metadata": {},
   "source": [
    "While most algorithms handle categorical data just fine, having too many categories will increase the complexity and reduce the generalizability of our model. When possible we should try to collapse categories into more managable features. Because education level has a natural progression to it (i.e., high levels of education indicate *more* education), I am converting education level into an ordinal variable with lower education levels represented by low number and higher education levels represented by higher numbers.  \n",
    "\n",
    "First, I will take a look at the values represented in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230dd521-e5bb-4da1-a8b8-d3c92ee1348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df['Education'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d749790-9818-4182-997e-13e9868c9050",
   "metadata": {},
   "source": [
    "Then, I will create a dictionary that maps those values to numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a90ad7-39eb-4835-a115-b4bb7edad86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_mapping = {\n",
    "    'Graduation': 3,\n",
    "    'PhD': 5,\n",
    "    'Master': 4,\n",
    "    '2n Cycle': 2,\n",
    "    'Basic': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ab1f7-181e-4f7a-b172-5f2d7f30d50b",
   "metadata": {},
   "source": [
    "Finally, I will create a new column of numeric values that are mapped to the original categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cdfa37-c35a-4d60-8470-619db7d2ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df['education_level'] = customer_df['Education'].map(education_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807b44d-8985-4682-9791-606f1ddbf69c",
   "metadata": {},
   "source": [
    "While marital status is often represented as married or not married, this dataset reflects a number of different living arrangements. To simplify, I am going to lump these statuses into groups of living alone or living together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc9109-a024-4e41-bd88-1249f41aa7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df['Marital_Status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4291a51d-a73c-4733-8667-2c6426e3af59",
   "metadata": {},
   "source": [
    "Here, I assign a 1 to the joint_household column if the marital status is 'Married' or 'Together'; otherwise, I set the joint_household column to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd9524-d27b-420a-8315-3e534322258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df['joint_household'] = customer_df['Marital_Status'].apply(lambda x: 1 if x in ['Married', 'Together'] else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c1727-c091-43b3-ac4a-162a3a21b882",
   "metadata": {},
   "source": [
    "From this joint_household column, I create a household_size column which represents the total number of people living under the same roof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26313a-f7dd-4fd1-9605-3060adc86ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df['household_size'] = (customer_df['joint_household'] + 1) + customer_df['total_children']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640a45d-f212-4adc-bd7e-7a3d2b765412",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_descriptives(customer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a05d207-f247-4a91-a9b2-a51e4a2f1470",
   "metadata": {},
   "source": [
    "Once I've finished engineering new features, I move on to feature selection. Because some of the features I created are designed to simplify the model, I will only be selecting features which I feel add unique knowledged about the customer into the model. Also, while some values are truly count values, I am including all values as numeric values for simplicity.  \n",
    "\n",
    "On the fianl line, I subset the full dataframe and create a new dataframe that includes only the features that I want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e6354-c492-4930-9e45-b55b35f6d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = []\n",
    "categorical_cols = []\n",
    "numeric_cols = ['Income', 'Recency', 'NumWebVisitsMonth', 'Complain', 'avg_spent', 'promotion_pct', 'promotion_response_rt', 'age', 'education_level', 'household_size'] + spent_cols + purchase_cols\n",
    "count_cols = []\n",
    "input_cols = [x for x in categorical_cols + numeric_cols + count_cols if x not in target_cols]\n",
    "data_cols = input_cols + target_cols\n",
    "df = customer_df[data_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343bd70-b9e6-4fb8-8c0d-a3bd86194050",
   "metadata": {},
   "source": [
    "Here we preview the features included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89f7b9-2002-457d-931f-2e88f2639c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee497127-45da-4d94-b455-d7ec9ef2856e",
   "metadata": {},
   "source": [
    "And a pairplot of the data will provide a high-level perspective on the shape of our data and whether there are any obvious problems with distributions or values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20fe2b4-daf7-4b2c-a710-7b941bcc2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sns.pairplot(df, corner=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f776ea21-5f78-4a5b-bcb0-bed9eac9cb60",
   "metadata": {},
   "source": [
    "#### Missing Values  \n",
    "Though it is a small view, you can clearly see outliers throughout the dataset. Almost every feature has at least one outlier. For example, The age column (column 8) has several individuals who are over 115. Before we can deal with outliers, we need to deal with missing values. This step comes before because almost all techniques for handling outliers requires analysis of the other values in the column.\n",
    "\n",
    "This line retrieves fields with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c90d6e-9e16-4492-996e-4653cc098e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1) | np.isinf(df).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e92cd-f9f6-411b-bb7b-2ca3860de9fa",
   "metadata": {},
   "source": [
    "I am electing to remove these fields. There are a few reasons for this, but they all boil down to preference. I prefer to remove missing values because I find all other methods problematic. For example, a common practice is to replace missing values with the mean for the column. Statistically, this makes the data point disappear (almost). However, there are several problems with this approach. First, it is not true and I prefer to deal with the truth. Second, when clustering, a large number of missing values placed at the mean could induce new clusters of customers because the other customer data point inidcate a non-average consumer who happens to be at the average on this particular field. Third, the data set is large enough that removing even 200 problematic records still leaves us with a sample of more than 2000 customers. \n",
    "\n",
    "So, I use the following line to replace all problematic values (dividing by zero produces inf values) with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c458d-dbd7-4b17-9f5c-e42810915f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70899625-821a-4b01-9da3-01827ef28a24",
   "metadata": {},
   "source": [
    "And then I drop those fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917090f1-0510-4e66-a0d8-2ff546066ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a094ae8a-9c6d-41cd-9218-650611dddb31",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "Outliers are dealt with in a similar way. Some prefer to replace the outlier with the mean of the column. Others may opt for something less drastic like replacing the outlier with the middle value of an exteme quartile thus leaving the value in a similar zone but closer to the overall distribution.\n",
    "\n",
    "I just like to get rid of them. The following line creates a mask which selects all values greater than 3.5 standard deviations from the mean. Statistically, this would leave >99% of our values untouched but would remove the more extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c494e-c805-4d8c-8bf7-41ef9ea83d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df[df.columns.difference(['Complain', 'promotion_response_rt'])].sub(df.mean()).div(df.std()).gt(3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284501db-6ae2-410e-bc58-ccfcd2b5b754",
   "metadata": {},
   "source": [
    "I then apply the mask and set those values to missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a6d905-f19b-4ef6-97c1-1c452e1ea765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.mask(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4537753-4d32-4138-b66e-5c2c9d291c06",
   "metadata": {},
   "source": [
    "This line highlights the fields that previously had missing values and now have missing values. It is always best to take a look at these values before removing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c460262-5b64-4efa-bbf3-9bb8f8cda72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1) | np.isinf(df).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328a885-ed8f-46de-8596-dcd48bcf96c6",
   "metadata": {},
   "source": [
    "Finally, I remove the new missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb9a38-3595-4dc9-a4dd-db5509444348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ef77d-c0fd-4987-a56f-c31587aaa120",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_descriptives(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427df409-d6f6-4cb2-9466-684bd5d8561e",
   "metadata": {},
   "source": [
    "### Model Specification\n",
    "#### Preprocessing\n",
    "As with the previous exercises, we will create some standard transformers to handle different kinds of data. Categorical variables must be transformed into number, count data must be log-transformed and scaled, and numeric data must be scaled. These pipelines ensure consistency. We may not always need all pipelines, but we will always want to handle these datatypes in a consistent way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd9f6d-474d-434d-b0ab-f02aa3ceb563",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_transformer = Pipeline(steps=[\n",
    "    ('log', FunctionTransformer(np.log1p)),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa3985-79ff-41a3-8e4f-8894b81d62f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a481f21-8cd6-4368-bb5f-e10f387f2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5921246c-f659-4068-9d16-d486b8f62208",
   "metadata": {},
   "source": [
    "#### Model Selection\n",
    "My preprocess only needs to handle numeric values because I transformed all categorical values into number and I've opted to treat all count values as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2c95d-9c92-4331-92fb-3410b7fb683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cont', numeric_transformer, numeric_cols),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01c4fb7-a509-4f41-b9e4-1cfef54f1d07",
   "metadata": {},
   "source": [
    "##### KMeans Clustering\n",
    "In the following lines, I create a KMeans base model and build a pipeline that transforms the features before feeding them into the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385afefc-ea48-47a5-a4df-5150b33438c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = KMeans(random_state=42)   # KMeans clustering\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c37792-29d0-459f-8a62-d35f3d6cbede",
   "metadata": {},
   "source": [
    "The KMeans clustering algorithm requires us to provide a number of clusters. We don't know how many clusters to define so we could guess or we could systematically try a variety of numbers of clusters to see how clustering peforms for each number of clusters. The following line calls a fuction I built which tests the model at a variety of numbers of clusters and reports the optimal number of clusters for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08296e-708f-43b6-a720-d602505cb084",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_clusters(pipeline, df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022552f7-acb9-4f26-9ff6-992de976dc00",
   "metadata": {},
   "source": [
    "The various metrics (Silhouette, Calinski-Harabasz, and Davies-Boulin) all assess cluster quality in various ways. The Silhouette score measures how similar an object is to its own cluster compared to other clusters, ranging from -1 to 1. A higher value indicates better-defined clusters. The Calinski-Harabasz score, evaluates the ratio of the sum of between-cluster dispersion to within-cluster dispersion. Higher values signify well-separated clusters. Lastly, the Davies-Bouldin score measures the average similarity ratio of each cluster with its most similar cluster, where lower values indicate better clustering quality. These scores together offer insights into the compactness and separation of clusters, aiding in the selection of an optimal clustering algorithm.\n",
    "\n",
    "All three metrics seem to agree that our data have 2 clusters so we're going to run the KMeans model with two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93582f-df2d-478d-954d-946d8b1a3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline['model'].n_clusters = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6f392-30db-4dee-81c2-14aea772c971",
   "metadata": {},
   "source": [
    "The clustering algorithms predict 'labels' which are indicative of the cluster to which each record belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3307e4e-c7cd-45c2-93b1-14349a0de607",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pipeline.fit_predict(df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605d2a0-390c-453c-99c4-6a1b9c3a9f73",
   "metadata": {},
   "source": [
    "We will add the labels to our dataframe so that we can take a look at the results and compare them to other clustering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32527434-4ba2-4e77-839a-bc6feab6eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kmeans_cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83faef-2ff6-4a2a-b8a9-228a264265e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sns.pairplot(df[numeric_cols + ['kmeans_cluster']], hue='kmeans_cluster', corner=True, palette=\"Set1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9d785-53fa-4ad5-81df-ff1f52ec6b04",
   "metadata": {},
   "source": [
    "##### Agglomerative Clustering\n",
    "In the following lines, I create an Agglomerative base model and build a pipeline that transforms the features before feeding them into the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505063cb-7cca-4a75-9c54-90391cf95b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AgglomerativeClustering()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cad771-dc53-4446-888e-ef43d7b90097",
   "metadata": {},
   "source": [
    "Again we will find the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643a150-131e-4366-b423-fcb0e134bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_clusters(pipeline, df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a37093-e30d-4319-9479-7e9634a34f5f",
   "metadata": {},
   "source": [
    "Here the metrics are inconsistent with two metrics opting for 2 clusters and 1 opting for 3 clusters. For variety, we will try a 3 cluster approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41941632-2ed7-4dad-a63b-62eee24c89d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline['model'].n_clusters = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73803960-6672-4170-bb5d-496155987be9",
   "metadata": {},
   "source": [
    "Again, we predict the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f046a6b-6cac-4862-896c-635f23376fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pipeline.fit_predict(df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a527c97-d442-4ac0-bd05-e55bade27d93",
   "metadata": {},
   "source": [
    "And save them to our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3fb6aa-080d-433d-8365-906a203e898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['agglomerative_cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc52fc-22e8-4919-a4c3-9e96a7d879f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sns.pairplot(df[numeric_cols + ['agglomerative_cluster']], hue='agglomerative_cluster', corner=True, palette=\"Set1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cbf985-969e-4906-86b9-c9be034aeaa9",
   "metadata": {},
   "source": [
    "##### DBScan Clustering\n",
    "In the following lines, I create an Agglomerative base model and build a pipeline that transforms the features before feeding them into the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd06be2-9910-4305-a7c1-4a3cc868ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DBSCAN()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9735f155-fc12-4a2d-a3a6-34dd865fcb41",
   "metadata": {},
   "source": [
    "The DBScan algorithm uses *epsilon* to predict clusters based on the density of the datapoints. So, we we search for optimal clusters, the function will run the model with various values of epsilon and report the best performing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15084b5-c723-42e2-b250-b0189e9e64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_clusters(pipeline, df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a4ab16-9da8-472c-b127-4476b9a13fa8",
   "metadata": {},
   "source": [
    "Here, an epison of 2.25 seems to proivde the best clustering performance so we'll set eps to 2.25. The DBScan algorithm also requires a min_samples hyperparameter that can affect model performance. Rather than testing various levels of min_samples, it is easiest to start with a min_samples value that is twice the number of features in your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7808d-127f-4219-b5ae-d83c78df5905",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline['model'].eps = 2.25\n",
    "pipeline['model'].min_samples = 2 * len(numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ccfd8b-bb98-415e-a580-961515b91e40",
   "metadata": {},
   "source": [
    "Again, we fit the model and predict the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a98fd-b455-4eb2-acf0-7d86588b87d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pipeline.fit_predict(df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79301b0-8473-4f54-88c4-90a83a743a84",
   "metadata": {},
   "source": [
    "Again, we add the labels to our dataframe for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2b703-8369-4156-b500-5bdbab4b58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dbscan_cluster'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b144e-97ae-4ff0-905c-bebdcb005e8f",
   "metadata": {},
   "source": [
    "We have no way of knowing how many clusters the DBScan algorithm produces so I'm going to summarize the counts in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49316d2-b257-4bdc-b62a-d22f0c85b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dbscan_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb3ab56-9a63-4ba0-bf54-6b2aa0ce913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sns.pairplot(df[numeric_cols + ['dbscan_cluster']], hue='dbscan_cluster', corner=True, palette=\"Set1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd3661-8c8f-40e0-b23e-f434a405c96a",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d4d68-a177-4d80-be7f-20e968938156",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_preformance(df[numeric_cols], df['kmeans_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9cd27-1a8d-4dcf-a9e4-fadaa330ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_preformance(df[numeric_cols], df['agglomerative_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1afe01a-6c06-417c-9c36-aa774e27c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_preformance(df[numeric_cols], df['dbscan_cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3cfb2e-54ee-4b80-a42c-063e9d337bc1",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction\n",
    "With a large number of features, overfitting becomes a problem. A common way of dealing with feature propogation is to cluster the features in much the same way we try to cluster records. This effectively redueces the total number of features by grouping like features together in a single feature.\n",
    "\n",
    "As with clustering, it is not clear exactly what the optimal number of features may be. The following function iterates through a variety of feature structures and reports metrics which will help us make an informed decion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d97e3-8ebd-482e-bc20-6b03f7e4e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_dimensions(df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02b306-2645-4bf7-a4a5-4ca3fddea831",
   "metadata": {},
   "source": [
    "A scree plot displays the eigenvalues associated with each principal component, often in descending order. By examining the plot, one can identify the \"elbow\" point where the curve starts to flatten, indicating that additional components contribute less to the overall variance and may be less meaningful. The cumulative variance plot, on the other hand, shows the cumulative percentage of total variance explained by the principal components. By interpreting this plot, you can determine the number of components needed to capture a desired amount of variance. Typically, you'll look for the point where the cumulative variance reaches an acceptable threshold, such as 90%, to decide how many principal components to retain for further analysis. These plots help in reducing dimensionality while preserving the most critical information from the dataset.\n",
    "\n",
    "To incorporate this information, we will add a Principal Components Analysis step to our modeling pipeline. We will start with a value of 4 which explains 65% of the variance and seems to be the point after which little additional variance is explained by subsequent components. This is subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38c131-169b-4745-8bdf-40c95dc42b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = PCA(n_components=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a175053-8c14-4b7f-8a80-26f4d3fb257d",
   "metadata": {},
   "source": [
    "Now we set our base model and build the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d2808-3dd6-40c3-b179-3ee43c15b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = KMeans(random_state=42)   # KMeans clustering\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dr', dr),\n",
    "    ('model', base_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477a717-4155-49ff-8879-56bac1752a92",
   "metadata": {},
   "source": [
    "Again, we will find the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1de240-ac4e-4362-915f-c5f6e64dcc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_clusters(pipeline, df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d8a11-673d-45c0-ab56-ef672172c7d6",
   "metadata": {},
   "source": [
    "And we will set the number of clusters to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e5fd0-daaa-4b93-bc62-179f3f18e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline['model'].n_clusters = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57804c-20ce-463b-8eaf-5b3af40f3278",
   "metadata": {},
   "source": [
    "Using those parameters, we will fit our model and predict the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca5b88-5e38-469d-9c1e-d813b559de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pipeline.fit_predict(df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c7ed3e-eb11-46c2-9c46-18b85b2b7597",
   "metadata": {},
   "source": [
    "Finally, we add the labels to our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309f395-5d9d-4121-9861-b8b256548189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kmeans_cluster_dr'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4155c-71b5-416c-8417-6be745792edb",
   "metadata": {},
   "source": [
    "With the new labels applied, we can assess the performance of our model with fewer features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9be014-61a3-4108-aa76-d33a97d55131",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_preformance(df[numeric_cols], df['kmeans_cluster_dr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cab9ac-8147-45ab-98e4-d8445ce3a70a",
   "metadata": {},
   "source": [
    "This approach reduces the number of features from 18 to 4 and maintains the same level of clustering performance. This means that our clustering results are far more likely to be generalizable and the model requires far less computing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3649627-637a-40ed-9355-6057528d2a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sns.pairplot(df[numeric_cols + ['kmeans_cluster_dr']], hue='kmeans_cluster_dr', corner=True, palette=\"Set1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf84743e-d7ec-4b56-b26a-9541bd769606",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The resutls of our clustering produces a 2-cluster solution. The results are good, but given the size of our sample and the nature of our data, a 2-cluster result is a little disappointing because it does not provide much insight about our customers. Instead, the analyses produced heavy-user/light-user clusters that one would expect to find in almost every retail location. It does not appear that there are different types of heavy-users nor does it appear that there are different types of light-users. This is a little surprising but it may be an artificact of the shopping categories we have at our disposal. It is possible the more shopping data would give us more insight into niche groups within our sample. Alternatively, we could build a new dataset by splitting our sample on some interesting column to look for emergant subgroups. For example, we could split the sample into groups of high vs. low web visists per month because it is likely easier to target online shoppers than it might be to target catelog or in-store customers. Then, we could look for clusters of shoppers in the high-web visits sample to see if there are any niche groups we could target with specialized promotion campaigns.\n",
    "\n",
    "## Clustering Exercise 2\n",
    "### Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a14e9-f9e9-421f-b0a5-108e222db1bd",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea1937-873d-417f-99dc-c03495471f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_df = pd.read_csv('data/health_departments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f32433f-aef8-41ce-bfe1-3850cedac5f9",
   "metadata": {},
   "source": [
    "### Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558c809-db7a-4a2b-9b0e-612162c5e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eba2ed-7855-4c66-8ad6-28998af8adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c39a39-7818-45e9-bd93-812a19e56442",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1100f0-27f8-4380-8072-ae12b0afef08",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867925ec-21b2-4cf2-9dcd-a478e00fdbea",
   "metadata": {},
   "source": [
    "Create a feature for followers relative to population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f6688-f364-449b-9c4b-aefd4766caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_df['followers_per_100k'] = hd_df['followers_count'] / (hd_df['state_population'] / 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566c0c3-65a9-411b-8ae4-d5bbcee4937d",
   "metadata": {},
   "source": [
    "Create a fo-fo ratio of following to followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ffad74-da08-4d4c-9ecf-fdf9aa01d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_df['fofo_ratio'] = hd_df['following_count'] / hd_df['followers_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e01be4-f6c0-4a06-b4b7-39b87369aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_descriptives(hd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e64acb-a8d8-4717-9a8a-cf1979096393",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = []\n",
    "categorical_cols = []\n",
    "numeric_cols = ['tweet_count', 'following_count', 'followers_per_100k', 'verified', 'age']\n",
    "count_cols = []\n",
    "input_cols = [x for x in categorical_cols + numeric_cols + count_cols if x not in target_cols]\n",
    "data_cols = input_cols + target_cols\n",
    "df = hd_df[data_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3621b8c7-cf34-4811-86c1-b7b5e99dcf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b786de2d-3bf4-4a75-8e6a-47ae1118d9b7",
   "metadata": {},
   "source": [
    "Check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd362def-76e0-4a4f-8bc5-7c675a4038f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, corner=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46817355-fa13-460c-a2e1-c56c27670db6",
   "metadata": {},
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361aa94-1568-46e5-86bf-5a4ac2e0831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1) | np.isinf(df).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683717c9-45dc-4f17-9b84-bebca5d03ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc302c3-989e-49d1-a8db-877d65f7bdcd",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fcf3e9-c120-4218-ac0b-bf40b88f039d",
   "metadata": {},
   "source": [
    "Do some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657e391-3e0b-4d58-85f8-0a321c102b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f59144-1b79-4908-9694-59ad880f2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cont', numeric_transformer, numeric_cols),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d8f38-ace2-4bf4-806d-f603488d0f1a",
   "metadata": {},
   "source": [
    "Specify KMeans model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf9ed0-6993-4942-acab-90a2ef171d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = KMeans(random_state=42)   # KMeans clustering\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29d2034-a1a3-4a6c-9890-57f5b71acd5e",
   "metadata": {},
   "source": [
    "Find optimal clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56348651-bf3e-450c-a2f6-22fd1f94a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_clusters(pipeline, df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5737c3d-591b-4fd4-85ff-f31c714acd36",
   "metadata": {},
   "source": [
    "Let's go with 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bdb4ed-f32d-47c8-80c8-7716a31f32a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline['model'].n_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5c4c9-3c8d-4502-9b22-7b0df69a1642",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pipeline.fit_predict(df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4b1a82-e606-4ba7-a03f-2de0557999a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kmeans_cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d37a96-c2fe-4eaf-af30-0a3f24ea647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AgglomerativeClustering()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "])\n",
    "\n",
    "find_optimal_clusters(pipeline, df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ddd4f-d95a-4f74-b5e2-2e768d3e34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline['model'].n_clusters = 2\n",
    "\n",
    "labels = pipeline.fit_predict(df[numeric_cols])\n",
    "\n",
    "df['agglomerative_cluster'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae49df-9b23-4971-b6a4-d1a60e66e310",
   "metadata": {},
   "source": [
    "DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225208d5-786d-4b3c-bbf6-994f82b8bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DBSCAN()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9cd67c-911d-40a1-876a-bd6b7696e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_clusters(pipeline, df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7b75a-47bf-439e-b3af-b2b22cf20192",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline['model'].eps = 2.5\n",
    "pipeline['model'].min_samples = 2 * len(numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f260b-564d-4958-b737-0b8177acc4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dbscan_cluster'] = pipeline.fit_predict(df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9bf62-c8cd-4df1-bbfc-77bffe6df8a3",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37fe035-c71c-433b-aa3d-c7e1c19d40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c8065-71ff-43ba-bcf5-2bf5ad8ff9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_preformance(df[numeric_cols], df['kmeans_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a905884a-31c8-4dd9-8fda-bbcb22ee1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_preformance(df[numeric_cols], df['agglomerative_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a209906-53b9-43f3-935a-25e8eb69f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_preformance(df[numeric_cols], df['dbscan_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f90da-9936-41fb-b10d-d39157b84eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[numeric_cols + ['agglomerative_cluster']], corner=True, hue='agglomerative_cluster', palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03c7f49-2459-4dc3-b8d5-0c3a42e43e7a",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
