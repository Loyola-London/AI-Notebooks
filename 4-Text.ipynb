{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa31697-cddd-4bd4-8797-229f99131f07",
   "metadata": {},
   "source": [
    "# Module 04 - Text\n",
    "## Natural Language Processing\n",
    "Natural Language Processing (NLP) is a branch of artificial intelligence (AI) that enables machines to understand, interpret, and generate human language. It combines computational linguistics with machine learning to process text or spoken words in a way that's meaningful. By breaking down language into smaller components—such as sentences, phrases, and even individual words—NLP models can analyze their structure and derive context. Tasks like language translation, sentiment analysis, and text summarization are all powered by NLP. Its significance lies in bridging the gap between human communication, which is inherently complex and nuanced, and machine comprehension.\n",
    "\n",
    "NLP is essential because it empowers machines to interact with humans more naturally and effectively. For instance, virtual assistants like Siri, Alexa, and Google Assistant rely on NLP to understand voice commands, answer questions, and execute tasks such as setting reminders or playing music. Similarly, search engines like Google use NLP to interpret queries and deliver the most relevant results by understanding the context and intent behind the words. Customer service chatbots also employ NLP to provide instant responses to user inquiries, improving efficiency and user experience. In the healthcare sector, NLP is used to analyze clinical notes, enabling faster diagnosis and better patient care. On social media, platforms like Facebook and Twitter utilize NLP for tasks like sentiment analysis, allowing brands to gauge public opinion and respond effectively. These real-world examples highlight how NLP is transforming our daily interactions with technology, making them seamless and intuitive.\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"https://thislondonhouse.com/Jupyter/Images/nlp.png\"></p>\n",
    "\n",
    "### Text Preprocessing \n",
    "Text preprocessing is a fundamental step in natural language processing (NLP) that prepares raw text data for analysis by machines. Since raw text is often noisy and unstructured, preprocessing cleans and standardizes it to improve the performance of machine learning models. Common techniques include removing stop words (e.g., \"and,\" \"the\"), punctuation, and special characters, as well as converting text to lowercase, stemming, and lemmatization to simplify words to their base forms. Additionally, tokenization divides the text into smaller units such as words or sentences for easier processing. Despite its importance, text preprocessing comes with challenges, such as determining which stop words to retain in context-specific tasks and balancing the trade-off between simplifying text and preserving its meaning.\n",
    "\n",
    "### Feature Extraction\n",
    "Feature extraction involves converting the preprocessed text into numerical representations that machine learning algorithms can understand. Techniques like Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and word embeddings (e.g., Word2Vec, GloVe, or BERT) are widely used for this purpose. Feature extraction helps to capture the semantics and context of the text, enabling more effective model training and predictions. However, it presents challenges like ensuring the representations are meaningful, especially for complex languages or highly ambiguous text. High-dimensional data generated by some methods, such as BoW, can also lead to issues like the curse of dimensionality, affecting model efficiency and accuracy. The choice of feature extraction method often depends on the specific task and data characteristics, making it a critical aspect of NLP workflows.  \n",
    "\n",
    "| Technique       | Description                                                                                 | Pros                                               | Cons                                                |\n",
    "|-----------------|---------------------------------------------------------------------------------------------|----------------------------------------------------|-----------------------------------------------------|\n",
    "| **Bag of Words (BoW)** | Represents text data by counting the occurrence of each word in the document. Each word is treated as an independent feature.                 | Simple and easy to implement.  Widely used and well-understood. Fast and computationally efficient.                     | Ignores word context and meaning. Results in sparse matrices. Limited to lexical matching.|\n",
    "| **TF-IDF**      | Weighs the frequency of words by their inverse document frequency. Reduces the impact of common words.                         | Captures important words based on their uniqueness. Better at handling irrelevant words than BoW. Provides interpretable features. | Ignores word context and meaning. Results in high-dimensional sparse matrices. Limited to lexical matching.                    |\n",
    "| **Word Embeddings** | Represents words as dense vectors in a continuous vector space based on their context and meaning. Techniques include Word2Vec, GloVe, FastText, and BERT.  | Captures semantic relationships between words. Handles polysemy and context effectively. Pre-trained models available for various domains.      | Requires significant computational resources.  Requires large corpora for training. May require fine-tuning for specific tasks.           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5af7a5-4bb1-455f-84a7-031832ccb78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from groq import Groq\n",
    "import datetime\n",
    "import pprint\n",
    "import statistics\n",
    "import os \n",
    "from dotenv import load_dotenv \n",
    "import json\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3afaa10-abe0-4d06-ac7f-99e87432f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def plot_feature_effects(clf, target_names, feature_names, top_n=20):\n",
    "    features = []\n",
    "    coefs = []\n",
    "    classes = []\n",
    "\n",
    "    try:\n",
    "        average_feature_effects = clf.coef_\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    if len(target_names) > 2:\n",
    "        # learned coefficients weighted by frequency of appearance\n",
    "\n",
    "        for i, label in enumerate(target_names):\n",
    "            coefs.extend(average_feature_effects[i][np.argsort(average_feature_effects[i])[-top_n:][::-1]])\n",
    "            features.extend(feature_names[np.argsort(average_feature_effects[i])[-top_n:][::-1]])\n",
    "            classes.extend([label] * top_n)\n",
    "    else:\n",
    "        for i, label in enumerate(target_names):\n",
    "            if i == 0:\n",
    "                coefs.extend(average_feature_effects[0][np.argsort(average_feature_effects[0])[:top_n][::-1]])\n",
    "                features.extend(feature_names[np.argsort(average_feature_effects[0])[:top_n][::-1]])\n",
    "                classes.extend([label] * top_n)\n",
    "            else:\n",
    "                coefs.extend(average_feature_effects[0][np.argsort(average_feature_effects[0])[-top_n:][::-1]])\n",
    "                features.extend(feature_names[np.argsort(average_feature_effects[0])[-top_n:][::-1]])\n",
    "                classes.extend([label] * top_n)\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({'features': features, \n",
    "                                          'coefs': coefs, \n",
    "                                          'classes': classes})\n",
    "\n",
    "    # Create the bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(y='features', x='coefs', hue='classes', data=feature_importance_df)\n",
    "    plt.title('Feature Coefficients by Class')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Coefficient Value')\n",
    "    plt.legend(title='Classes')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def classifier_performance(y, y_pred, labels_dict=None):\n",
    "    accuracy = metrics.accuracy_score(y, y_pred)\n",
    "    precision = metrics.precision_score(y, y_pred, average='weighted')\n",
    "    recall = metrics.recall_score(y, y_pred, average='weighted')\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y, y_pred)\n",
    "    f1 = metrics.f1_score(y, y_pred, average='weighted')\n",
    "    report = metrics.classification_report(y, y_pred, target_names=[labels_dict[i] for i in sorted(\n",
    "        labels_dict.keys())] if not labels_dict is None else np.unique(y_pred))\n",
    "\n",
    "    # Display the confusion matrix with custom labels\n",
    "    conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[labels_dict[i] for i in sorted(\n",
    "        labels_dict.keys())] if not labels_dict is None else np.unique(y_pred))\n",
    "    disp.plot(cmap=plt.cm.Greens)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(report)\n",
    "    plt.show()\n",
    "\n",
    "def get_prepared_remarks(transcript):\n",
    "    prepared_comments_started = False\n",
    "    prepared_comments = []\n",
    "    for transcript_line in transcript.split(\"\\n\"):\n",
    "        speaker = transcript_line[:transcript_line.find(\":\")]\n",
    "        if speaker.lower() == 'operator':\n",
    "            if prepared_comments_started:\n",
    "                # prepared comments have concluded\n",
    "                break\n",
    "            else:\n",
    "                # meeting has just begun\n",
    "                prepared_comments_started = True\n",
    "        else:\n",
    "            prepared_comments.append(transcript_line)\n",
    "\n",
    "    return \"\\n\".join(prepared_comments)\n",
    "\n",
    "def is_question(prompt):\n",
    "    # Check if the prompt ends with a question mark\n",
    "    if prompt.strip().endswith('?'):\n",
    "        return True\n",
    "\n",
    "    # Check for common question words\n",
    "    question_words = [\"who\", \"what\", \"where\", \"when\", \"why\", \"how\"]\n",
    "    for word in question_words:\n",
    "        if word in prompt.lower().split():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def retrieve_document(query, documents, top_n=3, query_method='TFIDF'):\n",
    "\n",
    "    if query_method == 'TFIDF':\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "        query_vec = vectorizer.transform([query])\n",
    "        cosine_similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    else:\n",
    "        model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        document_embeddings = model.encode(documents, convert_to_tensor=True)\n",
    "\n",
    "        query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "        cosine_similarities = util.pytorch_cos_sim(query_embedding, document_embeddings).flatten()\n",
    "        cosine_similarities = cosine_similarities.numpy()\n",
    "\n",
    "    top_indices = np.argsort(cosine_similarities)[::-1][:top_n]  # Get indices of top_n most similar documents\n",
    "    return [i for i in top_indices]\n",
    "\n",
    "def build_user_prompt(df):\n",
    "    user_input = input(\"\")\n",
    "    if user_input.lower() in (\"\", \"exit\", \"quit\", \"bye\", \"goodbye\"):\n",
    "        return False\n",
    "\n",
    "    if is_question(user_input):\n",
    "        # Looks like a question...searching for supporting documents...\n",
    "        top_document_rows = retrieve_document(user_input, df['Content'].tolist(), top_n=3, query_method='TFIDF')\n",
    "        search_df = df.iloc[top_document_rows]\n",
    "\n",
    "        context = \"\\n\\n\".join([f\"Document {i}:\\n{doc}\" for i, doc in enumerate(search_df['Content'])])\n",
    "        user_prompt = f\"{user_input}\\n\\nRelevant documents:\\n{context}\"\n",
    "    else:\n",
    "        user_prompt = user_input\n",
    "        \n",
    "    return {'input': user_input, 'prompt': user_prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe3873f-3b7b-4b04-ad3e-111253c90e52",
   "metadata": {},
   "source": [
    "## Text Exercise 1\n",
    "### Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bd66b5-e70d-4974-93f3-0d8bc5449ddf",
   "metadata": {},
   "source": [
    "Detecting misinformation on social media is crucial because it helps preserve the integrity and reliability of information shared online. Misinformation can spread rapidly, leading to confusion, fear, and mistrust among users. It can influence public opinion, harm individuals and communities, and even impact critical decisions such as those related to health, safety, and elections. By detecting and addressing misinformation, we can promote informed decision-making, protect public health, and maintain a more accurate and trustworthy information ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d3a95-71b3-4963-a547-babc5f50ca42",
   "metadata": {},
   "source": [
    "### Data Collection/Selection\n",
    "We will be loading data from a twitter dataset. \n",
    "\n",
    "Data are orgnized in tabular format with each record representing an individual user. The target variable is 'Role' which represents whether the individual supported or refuted misinformation. The data contain the following columns:  \n",
    "| Variable Name              | Role       | Data Type  | Description                                    |\n",
    "|----------------------------|------------|------------|------------------------------------------------|\n",
    "| id                         | Feature    | String     | Unique identifier for each user                |\n",
    "| screen_name                | Feature    | String     | User's screen name or handle                   |\n",
    "| verified                   | Feature    | Boolean    | Whether the user is verified                   |\n",
    "| age                        | Feature    | Integer    | User's age of account                          |\n",
    "| description                | Feature    | String     | User's profile description                     |\n",
    "| tweet_count                | Feature    | Integer    | Total number of tweets posted by the user      |\n",
    "| listed_count               | Feature    | Integer    | Number of lists the user is a member of        |\n",
    "| follower_count             | Feature    | Integer    | Total number of followers the user has         |\n",
    "| friend_count               | Feature    | Integer    | Total number of friends the user has           |\n",
    "| mindset_period             | Feature    | String     | Period during which the mindset was analyzed   |\n",
    "| WC                         | Feature    | Integer    | Word count in user's tweets                    |\n",
    "| fofo_ratio                 | Feature    | Float      | Ratio of certain words or phrases              |\n",
    "| mindset_count              | Feature    | Integer    | Count of messages influencing mindset          |\n",
    "| mindset_text               | Feature    | String     | Text associated with the user's mindset        |\n",
    "| Participation              | Feature    | Boolean    | Total participation                            |\n",
    "| Support                    | Feature    | Boolean    | Count of supporting messages                   |\n",
    "| Refute                     | Feature    | Boolean    | Count of refuting messages                     |\n",
    "| Morality                   | Feature    | Integer    | Count of morality messages                     |\n",
    "| SafetyConcerns             | Feature    | Integer    | Count of safety concerns messages              |\n",
    "| CivilLiberties             | Feature    | Integer    | Count of civil liberties messages              |\n",
    "| Conspiracy                 | Feature    | Integer    | Count of conspiracy theories messages          |\n",
    "| Original                   | Feature    | Integer    | Count of original tweets                       |\n",
    "| Retweet                    | Feature    | Integer    | Count of retweets                              |\n",
    "| Favorite                   | Feature    | Integer    | Count of favorited tweets                      |\n",
    "| Reply                      | Feature    | Integer    | Count of reply tweets                          |\n",
    "| Quote                      | Feature    | Integer    | Count of quote tweets                          |\n",
    "| Role                       | Target     | String     | Role of the user in the discussion             |\n",
    "| Myth                       | Feature    | String     | Type of myth the user discusses                |\n",
    "| Supported                  | Feature    | Boolean    | Whether the user supported misinformation      |\n",
    "| SupportedMorality          | Feature    | Boolean    | Whether supported morality misinformation      |\n",
    "| SupportedSafetyConcerns    | Feature    | Boolean    | Whether supported safety misinformation        |\n",
    "| SupportedConspiracy        | Feature    | Boolean    | Whether supported conspiracy misinformation    |\n",
    "| SupportedCivilLiberties    | Feature    | Boolean    | Whether supported civil liberty misinformation |\n",
    "| Security                   | Feature    | Integer    | Extent exhibited value of security             |\n",
    "| Conformity                 | Feature    | Integer    | Extent exhibited value of conformity           |\n",
    "| Tradition                  | Feature    | Integer    | Extent exhibited value of tradition            |\n",
    "| Benevolence                | Feature    | Integer    | Extent exhibited value of benevolence          |\n",
    "| Universalism               | Feature    | Integer    | Extent exhibited value of universalism         |\n",
    "| SelfDirection              | Feature    | Integer    | Extent exhibited value of self-direction       |\n",
    "| Stimulation                | Feature    | Integer    | Extent exhibited value of stimulation          |\n",
    "| Hedonism                   | Feature    | Integer    | Extent exhibited value of hedonism             |\n",
    "| Achievement                | Feature    | Integer    | Extent exhibited value of achievement          |\n",
    "| Power                      | Feature    | Integer    | Extent exhibited value of power                |\n",
    "\n",
    "The following line will load the data as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8723a5f-9aa3-4038-87c5-e9b5f8bb31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = pd.read_csv(\"data/twitter_misinformation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e9b9c-1642-4bad-bf6e-2f248ba1019a",
   "metadata": {},
   "source": [
    "### Data Profiling  \n",
    "Once the data are loaded, we need to profile the data and prepare it for analysis. This typically involves several steps that may include handling missing data, exploring data, feature selection, among others. The steps will vary depending on the dataset and the business problem, but profiling always precedes model building.  \n",
    "\n",
    "For analyzing text, you usually want to engage in a text-cleaning process. Fortunately, this dataset is pretty clean and what cleaning remains will be handled in feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f5a86-f157-44fe-a22a-f29c3be8c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490556e-e22a-455d-a415-684a1a301199",
   "metadata": {},
   "source": [
    "Here are all of the features available. We could build a complex model for predicting supporing misinformation, but for this exercise we will focus only on the user's prior tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aeee14-4f3f-4033-ad4b-982dcbe2cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9481b7-4406-4938-84de-7c99aa9910cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.hist(figsize=(12, 10), bins=30, edgecolor=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe938d0-8e12-487e-890a-20d7287d3c78",
   "metadata": {},
   "source": [
    "Here is a sample of some user's prior tweets. Looking at these results and the WC histogram will give you a feel for roughly how much text is involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ccfd2b-b1fd-4872-90bd-9be1ab58e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['mindset_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933af0df-8550-4006-85e6-0feea3b96518",
   "metadata": {},
   "source": [
    "These are the groups we will classify. Support indicates the user tweeted messages supporting misinformation and refute inidicates users who tweets messages refuting misinformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ee7fe-a3b1-4586-8c3b-806d2597123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['Role'].value_counts().plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a87b19-1317-42ec-9f07-d98d8cc58a9b",
   "metadata": {},
   "source": [
    "As with previous exercises, we will subset our data into a dataframe with only features that will be needed for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc31209f-31de-4ef4-8e92-facf101a072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['Role']\n",
    "input_cols = ['mindset_text']\n",
    "data_cols = input_cols + target_cols\n",
    "\n",
    "df = twitter_df[data_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d140a17-e83e-4bd4-9d88-8d2444ca1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8b3bd-a541-45a3-8515-13014c5537f3",
   "metadata": {},
   "source": [
    "There are no missing values, but we'll drop NA out of habit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d06e30-d97f-4c15-9655-3fc27fe6af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7fa439-4aa3-4c3c-8435-33c6bdceab98",
   "metadata": {},
   "source": [
    "Finally, we will create our test/train split and then we will move on to model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8930f6-34aa-4dca-9ce2-d37cd4343bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[input_cols], df[target_cols], test_size=0.25, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec053ac8-9615-4d59-8471-511762ab9a43",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b05d3d-577a-4f5d-9f60-77125d52feb2",
   "metadata": {},
   "source": [
    "Unlike previous exercises where we transformed features based on their datatype, in this exercise, we will use a vectorizer to turn the tweet text into a vector of numeric values which indicate word usage patterns. In effect, the vectorizer will convert the single feature 'mindset_text' into a larger number of features with each feature representing a word or part of a word that is found within the mindset_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d403ac-c955-45c0-9f07-4e30931ba2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=5, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89e9db-f90c-40e6-a9b8-a612503e4695",
   "metadata": {},
   "source": [
    "Since we are conducting a classification analysis, we will start with a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb304a8e-8c9f-4e04-a52b-607cc193508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LogisticRegression(C=5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c7a83-04ca-43f1-a9e3-e8ad7d7621d0",
   "metadata": {},
   "source": [
    "As with previous exercises, we compile these elements into a pipeline of steps. This helps ensure consistency across models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3ba91-d67b-42ad-89e5-95d54c6a76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),  # Step 1: Transform text data using TF-IDF\n",
    "    ('classifier', LogisticRegression())  # Step 2: Train a classifier on the TF-IDF features\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e69f71-96a9-47bc-8f6b-e95991a03b2d",
   "metadata": {},
   "source": [
    "First we will fit the model and then we will assess fit performance by predicting results from our testing data. This line may look a little different from previous exercises. The difference lies in the np.ravel(X_train). We have to use ravel() in this case because there is only one input feature (the vectorizer turns that one feature into many features) and ravel() is used to format single column dataframes into an appropriately structure array. We've used ravel() on the y values in all previous exercises as we have always had only one dependent variable, but this is the first time we have had only one independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b648c-25a2-4388-9b77-48b69df0a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipeline.fit(np.ravel(X_train), np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f48daf2-a828-4473-b353-578cb4bd3f9c",
   "metadata": {},
   "source": [
    "Next, we predict the results and investigate the classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b433ca8-ee09-4b3f-afa9-9479d7b24f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_predicted = logistic_pipeline.predict(np.ravel(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d33b04e-8a1c-46ba-9a7d-26383e6f9182",
   "metadata": {},
   "source": [
    "Surprisingly, the classifier proves to be quite capable (81% accuracy) of discriminating between supporters and refuters of misinformation based on their prior tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f760d49-5dc1-440b-aab3-d19b616c2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_performance(y_test, logistic_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a73892b-dda0-48b2-9390-43a526dd4716",
   "metadata": {},
   "source": [
    "The following function will plot the 15 features that are most predictive of supporting/refuting misinformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7008422-a69a-4773-9201-66da7d68386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_effects(logistic_pipeline['classifier'], np.unique(df['Role']), logistic_pipeline['tfidf'].get_feature_names_out(), 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf53897-3713-4f6e-9afd-548ac9454421",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c8771-0178-4462-9c25-67ab1b4a28b6",
   "metadata": {},
   "source": [
    "The results above are impressive, but there are other classifiers that may even improve on these results. In the following code block, a tuple of classifieres is created. Then the same steps as above are duplicated inside of a loop so that each model is evaluated indpendently. The loop also calculates how long it takes to train/test each model so that we can assess not only accuracy but speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c127a5-e45e-4410-a1b0-506634afeaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "classifiers = ((DummyClassifier(), \"Dummy Classifier\"),\n",
    "               (LogisticRegression(C=5, max_iter=1000), \"Logistic Regression\"),\n",
    "               (RidgeClassifier(alpha=1.0, solver=\"sparse_cg\"), \"Ridge Classifier\"),\n",
    "               (KNeighborsClassifier(n_neighbors=100), \"kNN\"),\n",
    "               (RandomForestClassifier(), \"Random Forest\"),\n",
    "               (LinearSVC(C=0.1, dual=False, max_iter=1000), \"Linear SVC\"),\n",
    "               (SGDClassifier(loss=\"log_loss\", alpha=1e-4, n_iter_no_change=3, early_stopping=True), \"log-loss SGD\",),\n",
    "               (NearestCentroid(), \"NearestCentroid\"),\n",
    "               (ComplementNB(alpha=0.1), \"Complement naive Bayes\"))\n",
    "\n",
    "for clf, name in classifiers:\n",
    "    print(\"=\" * 80)\n",
    "    print(name)\n",
    "    print(\"_\" * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),  \n",
    "        ('classifier', clf) \n",
    "    ])\n",
    "    pipeline.fit(np.ravel(X_train), np.ravel(y_train))\n",
    "\n",
    "    train_time = time() - t0\n",
    "    print(f\"train time: {train_time:.3}s\")\n",
    "\n",
    "    t0 = time()\n",
    "    y_pred = pipeline.predict(np.ravel(X_test))\n",
    "    test_time = time() - t0\n",
    "    print(f\"test time:  {test_time:.3}s\")\n",
    "    classifier_performance(y_test, y_pred, {0: 'Refute', 1: 'Support'})\n",
    "    plot_feature_effects(pipeline['classifier'], np.unique(df['Role']), pipeline['tfidf'].get_feature_names_out(), 15)\n",
    "    print()\n",
    "    if name:\n",
    "        clf_descr = str(name)\n",
    "    else:\n",
    "        clf_descr = clf.__class__.__name__\n",
    "\n",
    "    results.append((clf_descr, metrics.accuracy_score(y_test, y_pred), train_time, test_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23687dce-4fb1-4905-8fcf-b3c593e45c95",
   "metadata": {},
   "source": [
    "Next we will plot the speed metrics to help us make the tradeoff between speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4541960-ec97-4dd3-990a-f506e09e4a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time)\n",
    "test_time = np.array(test_time)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\n",
    "ax1.scatter(score, training_time, s=60)\n",
    "ax1.set(\n",
    "    title=\"Score-training time trade-off\",\n",
    "    yscale=\"log\",\n",
    "    xlabel=\"test accuracy\",\n",
    "    ylabel=\"training time (s)\",\n",
    ")\n",
    "ax2.scatter(score, test_time, s=60)\n",
    "ax2.set(\n",
    "    title=\"Score-test time trade-off\",\n",
    "    yscale=\"log\",\n",
    "    xlabel=\"test accuracy\",\n",
    "    ylabel=\"test time (s)\",\n",
    ")\n",
    "\n",
    "for i, txt in enumerate(clf_names):\n",
    "    ax1.annotate(txt, (score[i], training_time[i]))\n",
    "    ax2.annotate(txt, (score[i], test_time[i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a13103e-04e9-4f9c-9f00-08c09e301992",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde8f4e-5d7e-41e7-be1a-f41b253d8fb1",
   "metadata": {},
   "source": [
    "The model performed very well and given the complexity of speech, it is hard to imagine improving on this model. However, the TFIDF vectorizer is based on word count and not semantic meaning. So, it is possible to imagine improved performance on edge cases where refuters use language that mimic supporters, but differs in how they use the language. To achieve this, we would need to use a semantically aware vectorizer which would greatly increase the processing requirements of our model.\n",
    "\n",
    "Further, this model's accuracy is narrow as it only applies to misinformation about covid-19 vacccines. Given the proliferation of misinformation and the speed at which misinformation flows and evolves, a static model would have limited utility.\n",
    "\n",
    "However, in the context of misinformation about covid-19 vacccines, our model performs admirably and would be a useful tool for identify rumors and myths swirling around the covid-19 vaccine. For example, the following collection of words could be fed into our model to predict the likely role this user would play in any conversation about vaccine misinformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465fd0c4-dac1-471b-91a0-ddbd0188667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mindset_text = \"can t be bothered Like I meant to but then I get to the webpage and I m like Nah I mean it s not like I even had to move That s not even lazy I don t know what it is God bless Regis Peace and love to all his family I loved Regis he came to my home to interview me many many years ago and to this day he called me Big Hasselhoff He I didn t even know there was supposed to be a possible hurricane Seriously tho did these hurricanes pop up out of nowhere I didn t hear anything about either of them 24 hours ago WTF is going on Hurricane Hanna makes landfall in Texas via So did I Regis is in the same category as Carson Superlative He was on our show a million times always the best guest we ever had Regis Philbin truly was the host with the most most joyful most entertaining most unpredictable amp one of the most ama FLAVOR FLAV R I P Regis Philbin Your death was a shocker We are gonna miss you here Thankx for all your hours on I m only like 7 minutes into the BillandTed3 ComicConAtHome thing and I m already making faces No idea what Brigette said at 4 minutes in about pronouns And you know nepotism is one of my pet peeves IDK I did see that slasher wedding movie without realizing the bride was someone s kid I didn t like the movie So we ll see As with most issues in this country what the media presents as black vs white is usually really rich vs poor Police are going to act completely differently in a rich neighborhood than a poor one In my opinion the media s focus on race is a deliberate attempt to divide people I like that they re all in their own phone booths and when they re talking the electricity goes to them Wait I didn t notice this before The band is called Preston Logan instead of Wyld Stallyns WTF So neither of these kids saw Bill amp Ted before That s disturbing Especially Weaving s kid Because her dad s money comes from being in movies with Ted Theodore Logan Who raised these children Always a warm greeting Regis greeted you like a happy surprise he was delighted to see A really sweet man he took One of my favorite interviews with Reg They ll never be another like him RIP my friend XO RIPRegis Now I m getting the bad vibe that I m not going to like this movie BECAUSE I don t have kids It seems like a lot of this is going to be about the kids Younger Weaving is awkwardly giddy What is that I call the beneficiaries of nepotism stuff like younger Weaving or younger Washington or younger Kravitz because I don t think their level of talent would have gotten them there on their own So you might as well call a spade a spade Oh wow 37 minutes in Keanu voluntarily spoke I m actually shocked Yeah that was kinda ok Not psyched about the movie tbqh Something s off  2Priceless It ll just come back like Talking Tina I know I like John Saxon but I don t think it s from ENTER THE DRAGON A quick scan of his IMDb page didn t help me either I must be forgetting something \"\n",
    "test_mindset_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844728f-563c-44ba-ae29-2bb47103b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipeline.predict(np.array([test_mindset_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04120bd0-99d6-47e0-956b-1d4c74b389cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mindset_text = 'BREAKING President has withdrawn the US from the World Health Organization if you support Ilhan Omar just called for the dismantling of the U S economy and political systems This socialist has got to go if schools should be fully open this fall I will give one of these coins away to one person who and Likes this post If this post I ll give a Raise your hand if you think we should withdraw from the United Nations next Anyone like Ilhan Omar who calls for the literal DISMANTLING of our whole US system should be immediately removed from o Rt if Bill de Blasio should be REMOVED as Mayor for destroying NYC Trump should have said keep schools closed Just like hydroxychloroquine Marquette Univ threatened to cancel 18 y o incoming freshman Samantha Pfefferle s admission just for posting a pro Tru We ve received so much support in our first 24 hours Help us break 1 000 followers and let s show America that BlueLivesMa It s hard to find this because you scrubbed it from your government website But I saved a picture of it for ya Ilhan Omar s campaign has now officially paid her own husband more than ONE MILLION dollars in the 2020 cycle She is be Now that the White House took control of the coronavirus numbers from the CDC it s going to be interesting to see what they if you trust President Trump more than Dr Fauci 147 Covid deaths recorded today in AZ 106 are death certificate matching which means they died with a if you believe Melania Trump is the most beautiful and most eloquent First Lady of ALL TIME The Left doesn t want you to share this video African American support keeps growing for Retweet if think Chris Wallace belongs on Fake News CNN and you re sick and damn tired of hearing his lying mouth on Fox Ne Joe Biden is basically the bad guy in every civil rights movie ever made I cannot believe that in the year 2020 Americ Rt if Dr Fauci should resign The Key to Defeating COVID 19 Already Exists We Need to Start Using It Opinion if you think we need a national Voter ID law passed before November Remember when pro athletes still loved America Americans will love baseball again when loves America again http '\n",
    "test_mindset_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c649d-a4f3-4bcf-a220-490ab4207c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipeline.predict(np.array([test_mindset_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a7802-6452-43a3-9922-e0354effd7ac",
   "metadata": {},
   "source": [
    "## Text Exercise 2\n",
    "In this exerise, we will be building an LLM-wrapper application. These steps will serve as a model for how we approach LLM-wrappers in the future.  \n",
    "\n",
    "### Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91a60e-eac0-410f-9550-644d37734ab9",
   "metadata": {},
   "source": [
    "Using artificial intelligence to analyze earnings transcripts can unlock valuable insights in an efficient and comprehensive manner. AI can quickly process vast amounts of financial data, identifying key themes, trends, and sentiment across multiple transcripts—a task that would be time-consuming and prone to human error if done manually. By leveraging natural language processing, AI can detect subtle changes in tone, language, or emphasis that might signal shifts in a company's strategy or outlook. Additionally, AI can cross-reference this information with broader market trends, providing a deeper contextual understanding. This level of analysis empowers investors, analysts, and decision-makers to make more informed choices with greater speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603be49e-bc5c-41c3-83db-e121320617c8",
   "metadata": {},
   "source": [
    "### Data Collection/Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14888633-3245-4857-88d3-a97aa9938058",
   "metadata": {},
   "source": [
    "For this exercise, I have download an earnings class transcript for Intel (ticker: INTC) from quarter 2 of 2024. This transcript will serve as the data source for our wrapper application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737bb3e-489f-4294-80ee-347e76099bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/intel_transcript.txt\") as file_pointer:\n",
    "    call_transcript = file_pointer.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f40ed-2a19-4ff4-ae5f-877f05f4499e",
   "metadata": {},
   "source": [
    "I have written a function (get_prepared_remarks) which scans through the transcript and extracts the comments that precede the Q&A session at the end of earnings calls. These remarks are often finely tuned and serve as indicators of the company's future performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb22408-4e0f-4156-b067-c9f904a429aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prepared_remarks(call_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d54d2-0810-4dd7-819b-cc34341bf220",
   "metadata": {},
   "source": [
    "### LLM Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550fb79e-4530-4ac9-be8d-389a96f75b14",
   "metadata": {},
   "source": [
    "In this exercise, the LLM is our intelligence, but we have to tell it what kind of intelligence to exhibit. The system_content instructs the system how to respond to user input, and the user_content prompt provides the user's request to the intelligent system.  \n",
    "\n",
    "I have preloaded the system profile, but you should experiment and see if you can get the LLM to attend to different points of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaacda1-0ffa-4a41-a2f8-61e892b64f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_content = \"\"\"\n",
    "    you are financial data analyst. \n",
    "    you need to read earnings calls transcripts, identify the attendees, and assess the content the call.\n",
    "    when assessing the conent of the call, pay close attention to the following:\n",
    "    tone (positive/negative)\n",
    "    risk (low risk/high risk)\n",
    "    spending (increasing/decreasing)\n",
    "    rate each on a scale of 1 (negative, high risk, decreasing spending) to 10 (positive, low risk, increasing spending). \n",
    "    for each, include a 1 sentence that highlights specific details justifying your rating\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0edd4c3-1b74-49f6-9264-a32a47d591ea",
   "metadata": {},
   "source": [
    "In this line of code, I trim the first 3000 words off of the transcript to accomodate groq's token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda99447-da89-4f06-a2fd-bf464270ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_content = \" \".join(get_prepared_remarks(call_transcript).split(\" \")[:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d1401f-c1f3-4ffc-bab2-4e229025c784",
   "metadata": {},
   "source": [
    "### Application Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284fdb7f-e3da-4d4d-9875-0e212a3757ab",
   "metadata": {},
   "source": [
    "LLM inference is a computationally expensive task. Though you can run LLM inference on your individual device, it will likely be slow (if it runs at all). For this reason, there are many emerging AI companies that have emerged for the purpose of providing inference compute. To be able to access these cloud resources, you will need to [sign up for API access](https://console.groq.com/login). We will use a free level of service, but there are paid levels. So it is important to protect your key. Once you have created an API key, you can add it as a variable to a variables.env file to obscure the key from your source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11543d2b-a0a0-48a7-9e68-6cf995d4a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = 'variables.env'\n",
    "\n",
    "load_dotenv(dotenv_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ccbfc2-02c0-4448-889f-8fbed2ec28dd",
   "metadata": {},
   "source": [
    "Here we load the environment variable from the variables.env file and pass it into the Groq library to establish a link to their inference resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b638e31-a05c-43fe-9f51-576a3748fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7502fe6-4994-4b3f-85b8-6abf9cbd12b6",
   "metadata": {},
   "source": [
    "Though this is a large block of code, it is actually only one line of code that simply makes a call to Groq's inferernce servers. This line initializes a chat request with embedded messages for the system and the user. The specific LLM model (llama-3.1-8b-instant) is defined, the temperature (creativity/randomness of the response) is set, the max number of tokens (max length of reply), stop commands and whether to stream the LLM's response or wait until the full response is available before transmitting it back to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d708e5-02ef-43c2-b318-1488733d1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    #\n",
    "    # Required parameters\n",
    "    #\n",
    "    messages=[\n",
    "        # Set an optional system message. This sets the behavior of the\n",
    "        # assistant and can be used to provide specific instructions for\n",
    "        # how it should behave throughout the conversation.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_content\n",
    "        },\n",
    "        # Set a user message for the assistant to respond to.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_content,\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    # The language model which will generate the completion.\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "\n",
    "    #\n",
    "    # Optional parameters\n",
    "    #\n",
    "\n",
    "    # Controls randomness: lowering results in less random completions.\n",
    "    # As the temperature approaches zero, the model will become deterministic\n",
    "    # and repetitive.\n",
    "    temperature=0.5,\n",
    "\n",
    "    # The maximum number of tokens to generate. Requests can use up to\n",
    "    # 32,768 tokens shared between prompt and completion.\n",
    "    max_tokens=1024,\n",
    "\n",
    "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
    "    # likelihood-weighted options are considered.\n",
    "    top_p=1,\n",
    "\n",
    "    # A stop sequence is a predefined or user-specified text string that\n",
    "    # signals an AI to stop generating content, ensuring its responses\n",
    "    # remain focused and concise. Examples include punctuation marks and\n",
    "    # markers like \"[end]\".\n",
    "    stop=None,\n",
    "\n",
    "    # If set, partial message deltas will be sent.\n",
    "    stream=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50494547-4893-4853-a216-ab564ee6593c",
   "metadata": {},
   "source": [
    "Now, we can print the repsonse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8809945-ad26-43b6-a222-2de3a42e7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the completion returned by the LLM.\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39fb648-7461-4a87-88e4-41d45ec9cb53",
   "metadata": {},
   "source": [
    "## Text Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b22d85d-2299-43fe-9980-ebe8b9e4c735",
   "metadata": {},
   "source": [
    "### Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf5ee0-8e7e-49b0-be54-b4a3c247c60b",
   "metadata": {},
   "source": [
    "An internal LLM chatbot with access to private documents can be an invaluable tool for improving efficiency, collaboration, and decision-making within an organization. By securely accessing and processing internal data, the chatbot can provide accurate, context-specific responses to employee queries, reducing the time spent searching through files or waiting for assistance from colleagues. It can synthesize information from multiple sources, summarize lengthy documents, and highlight key insights, enabling users to quickly grasp complex topics. Additionally, such a chatbot can offer personalized recommendations and support tailored to the unique needs of the organization, fostering innovation and improving productivity—all while ensuring the confidentiality of sensitive information remains intact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd3ae6-cf0b-4326-b6c9-5775b61c3f16",
   "metadata": {},
   "source": [
    "### Data Collection/Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fdf77f-91a6-4016-ad3a-4ba30f5d2556",
   "metadata": {},
   "source": [
    "For this exercise, I have scraped several internal Loyola websites. These websites are commonly accessed by new students and typically provide technical information to Loyola students, faculty, and staff. Because these documents are private, search engines (and by extension LLMs) are unaware of the details of these documents. Without such details, external tools are of little help to internal stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da85c7-1b75-4735-bfa0-cdfc120d88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/loyola_documents.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60123574-1198-4d03-b881-02c9e55593ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5afc4b-da8b-4399-b21f-ad3e2796e602",
   "metadata": {},
   "source": [
    "### LLM Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f11a64-0eef-4c77-a23f-d87ed7377d3e",
   "metadata": {},
   "source": [
    "Here I define the behavior of the LLM system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c5cb77-5b87-4146-80e9-f1d74e485520",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_content = \"\"\"\n",
    "    you are a loyola university maryland chat bot.\n",
    "    you are aware of the jesuit values and you emulate those as you answer student questions about university life. \n",
    "    if your response includes details from a document, please summarize the relevant points.\n",
    "    do not refer to the documents in your response.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa4afb-d3c5-43b6-9e5a-b77c1fe2c135",
   "metadata": {},
   "source": [
    "In this code, I let the user make a request to the LLM. This is done to illustrate the difference between base LLM responses and contextually aware responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5461db8f-7571-48b0-9ee3-58ee1c311c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = input(\">\")\n",
    "user_content = {'input': user_prompt, 'prompt': user_prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cd239-9cff-44de-b47f-87052b3f76e9",
   "metadata": {},
   "source": [
    "In this code, I set the user content to the result of the build_user_prompt() function. This is a function that prompts the user for a message to send to the chatbot. It tries to assess whether the message contains a question. If it does contain a question, the function then searches for relevant documents to pass to the LLM to aid it in its response. This code is commented out at the start. To see how the LLM response differs when context is added, uncomment this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ae247-3f3b-4361-8365-a132c2c01481",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_content = build_user_prompt(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c548e-fbe5-476d-858c-9a1d0b8a83da",
   "metadata": {},
   "source": [
    "### Application Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f691cf87-4968-4b78-a4ab-024c299da601",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\x1B[1m\\x1b[33m{'-'*20}USER{'-'*20}\\x1B[0m\")\n",
    "print(user_content['input'])\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_content\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_content['prompt'],\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stop=None,\n",
    "    stream=False,\n",
    ")\n",
    "print(f\"\\x1B[1m\\x1b[32m{'-'*20}BOT{'-'*20}\\x1B[0m\")\n",
    "print(f\"{chat_completion.choices[0].message.content}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
