{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b70c2ea-6780-4fb8-aaac-21240cbf840d",
   "metadata": {},
   "source": [
    "# Module 05 - Images\n",
    "## Computer Vision\n",
    "Computer vision is a field of artificial intelligence that focuses on enabling machines to interpret and understand visual data from the world, such as images and videos. At its core, computer vision seeks to replicate human visual perception by using algorithms to process, analyze, and extract meaningful information from visual content. This could involve recognizing objects, detecting patterns, identifying faces, or even interpreting handwritten text. The field leverages advancements in machine learning and deep learning to make sense of complex visual data, transforming how machines \"see\" and interact with their environment. By converting raw pixels into actionable insights, computer vision is a bridge between digital systems and the physical world.\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"https://thislondonhouse.com/Images/computer_vision.png\"></p>\n",
    "\n",
    "Increasingly, businesses are turning to computer vision to automate tasks and enhance decision-making. For instance, in retail, it is used for inventory management and cashier-less checkout systems. In healthcare, computer vision powers diagnostic tools that analyze medical images, improving accuracy and speed. The field also finds applications in agriculture, manufacturing, and autonomous vehicles, where it monitors crops, detects defects in products, and enables safe navigation, respectively. By harnessing the potential of computer vision, businesses can drive innovation, optimize operations, and create new opportunities, all while delivering better value to their customers. This transformative capability exemplifies the power of AI to shape the future of industries.\n",
    "\n",
    "### Classification, Detection, Recognition\n",
    "The ability of computers to see is powered by a range of techniques in computer vision, primarily driven by advancements in machine learning and deep learning. Convolutional Neural Networks (CNNs) are at the forefront, designed to mimic human visual processing by using layers of filters to detect features such as edges, textures, and shapes in images. Techniques like object detection enable computers to locate and identify multiple objects within a scene, while semantic segmentation breaks an image into regions for pixel-level understanding. Optical character recognition (OCR) enables the interpretation of text in visual data, and image classification assigns labels to entire images based on their content. Additionally, methods like feature extraction and keypoint detection are used to match patterns or track movements. Combined, these techniques allow computers to analyze and interpret visual information, making them invaluable across various applications.\n",
    "\n",
    "Different techniques in computer vision excel in various tasks based on their design and focus, each bringing unique strengths to the table. For example, Convolutional Neural Networks (CNNs) are highly effective for image classification and feature extraction due to their ability to detect patterns and hierarchical features in visual data. Object detection techniques, such as YOLO (You Only Look Once) and Faster R-CNN, specialize in identifying and locating multiple objects within an image or video, offering speed and precision for real-time applications like autonomous driving. In contrast, semantic segmentation techniques like U-Net and Mask R-CNN provide a pixel-level understanding of images, useful in fields like medical imaging and agriculture where detailed analysis is critical.\n",
    "\n",
    "Meanwhile, Optical Character Recognition (OCR) is tailored for extracting and interpreting text from images, making it invaluable for document processing and automation. Feature-matching methods, such as Scale-Invariant Feature Transform (SIFT) and Speeded-Up Robust Features (SURF), are particularly useful for image stitching or 3D reconstruction by recognizing and aligning key points between images. The choice of technique often depends on the specific goals, computational resources, and accuracy requirements of the task, illustrating the diversity and adaptability of approaches within the realm of computer vision.\n",
    "\n",
    "## Synthetic Data\n",
    "Synthetic data is a powerful tool in machine learning, used to augment datasets, reduce bias, and train models more effectively when real-world data is scarce, expensive, or challenging to collect. By generating artificial yet statistically relevant data, synthetic datasets can simulate various scenarios and improve model generalization. This is particularly helpful in applications like healthcare, finance, and autonomous systems, where obtaining real data might involve privacy concerns, high costs, or safety risks. Synthetic data also aids in balancing datasets, mitigating issues like class imbalance, and creating diverse, representative training samples.\n",
    "\n",
    "In the context of image classification, synthetic data generation often involves techniques such as data augmentation, where transformations like rotation, scaling, flipping, and color adjustments are applied to existing images to produce variations. More advanced methods include using generative adversarial networks (GANs) or 3D rendering to create entirely new images based on the properties of the original dataset. This can be especially beneficial for rare or underrepresented classes in the dataset, as it provides additional samples to improve the model's performance and reduce overfitting. The key advantage is that synthetic data expands the scope of the dataset without requiring manual collection or labeling efforts\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"https://thislondonhouse.com/Images/hotdog_left.png\" width=40%>&nbsp;<img src=\"https://thislondonhouse.com/Images/hotdog_right.png\" width=40%></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729cd47-b339-4e83-affe-58c54ce575b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "import random\n",
    "import zipfile\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.layers import Dense, Input, MaxPooling2D, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6f820-683d-42ff-996a-d926243a2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "def plot_descriptives(df):\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Plot setup\n",
    "    num_plots = len(numeric_cols) + len(categorical_cols)\n",
    "    cols = 3\n",
    "    rows = int(np.ceil(num_plots / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Numeric columns: histograms\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        sns.histplot(df[col], bins=10, ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Categorical columns: bar charts\n",
    "    for j, col in enumerate(categorical_cols, start=len(numeric_cols)):\n",
    "        sns.countplot(x=df[col], ax=axes[j])\n",
    "        axes[j].set_title(f'Counts of {col}')\n",
    "        axes[j].set_xlabel(col)\n",
    "        axes[j].set_ylabel('Count')\n",
    "        axes[j].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Remove unused axes\n",
    "    for k in range(num_plots, len(axes)):\n",
    "        fig.delaxes(axes[k])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def load_data_from_directory(base_dir, include_flipped_images=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through each class folder\n",
    "    for class_name in os.listdir(base_dir):\n",
    "        class_dir = os.path.join(base_dir, class_name)\n",
    "\n",
    "        if os.path.isdir(class_dir):  # Ensure it is a directory\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                file_path = os.path.join(class_dir, file_name)\n",
    "\n",
    "                try:\n",
    "                    # Open the image, preprocess it, and add to dataset\n",
    "                    with Image.open(file_path) as img:\n",
    "                        img = img.resize((128,128))\n",
    "                        img_array = np.array(img)  # Convert to a NumPy array\n",
    "                        data.append(img_array)\n",
    "                        labels.append(class_name)  # Use the folder name as the label\n",
    "                        if include_flipped_images:\n",
    "                            flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                            flipped_img_array = np.array(flipped_img)  # Convert flipped image to a NumPy array\n",
    "\n",
    "                            # Append the flipped image and label\n",
    "                            data.append(flipped_img_array)\n",
    "                            labels.append(class_name)  # The label remains the same\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {file_path}: {e}\")\n",
    "\n",
    "    return np.array(data, dtype='float32')/255.0, np.array(labels)\n",
    "\n",
    "def load_random_images(folder, num_images):\n",
    "    images = []\n",
    "    if os.path.isdir(folder):\n",
    "        all_images = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(('png', 'jpg', 'jpeg'))]\n",
    "        sampled_images = random.sample(all_images, min(len(all_images), num_images))\n",
    "        images.extend(sampled_images)\n",
    "    return images\n",
    "\n",
    "def plot_images_grid(images, grid_size):\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i < len(images):\n",
    "            img = Image.open(images[i])\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def get_image(image_url):\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = img.resize((128, 128))  # Example for models like ResNet or VGG\n",
    "\n",
    "    # Convert to a NumPy array and normalize pixel values\n",
    "    img_array = np.array(img)  # Scale pixel values to [0, 1]\n",
    "    data = [img_array]\n",
    "    return np.array(data, dtype='float32')/255.0\n",
    "\n",
    "def flatten_images(X):\n",
    "    return X.reshape(X.shape[0], -1)\n",
    "\n",
    "def to_grayscale(images):\n",
    "    return np.array([rgb2gray(image) for image in images])\n",
    "\n",
    "def extract_hog_features(images, pixels_per_cell=(8, 8), cells_per_block=(2, 2), orientations=9):\n",
    "    return np.array([hog(image, \n",
    "                         pixels_per_cell=pixels_per_cell, \n",
    "                         cells_per_block=cells_per_block, \n",
    "                         orientations=orientations, \n",
    "                         block_norm='L2-Hys') for image in images])\n",
    "\n",
    "def create_sequential_model(dims, metric):\n",
    "    print(dims)\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=dims))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(10))\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=metric)\n",
    "    return model\n",
    "\n",
    "def classifier_performance(y, y_pred, labels_dict=None):\n",
    "    accuracy = metrics.accuracy_score(y, y_pred)\n",
    "    precision = metrics.precision_score(y, y_pred, average='weighted')\n",
    "    recall = metrics.recall_score(y, y_pred, average='weighted')\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y, y_pred)\n",
    "    f1 = metrics.f1_score(y, y_pred, average='weighted')\n",
    "    report = metrics.classification_report(y, y_pred, target_names=[labels_dict[i] for i in sorted(\n",
    "        labels_dict.keys())] if not labels_dict is None else np.unique(y_pred))\n",
    "\n",
    "    # Display the confusion matrix with custom labels\n",
    "    conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[labels_dict[i] for i in sorted(\n",
    "        labels_dict.keys())] if not labels_dict is None else np.unique(y_pred))\n",
    "    disp.plot(cmap=plt.cm.Greens)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(report)\n",
    "    plt.show()\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bfe35d-a312-4569-ae3d-119af12a3c70",
   "metadata": {},
   "source": [
    "## Image Exercise 1\n",
    "### Business Problem\n",
    "Being able to discriminate between types of objects is an essential first step to teaching a computer how to make sense of an image. If we are able to effectively teach a computer to discriminate between different types of images, we could then build subsequent tools that leverage these ablilities and automate processes that previously required human intervention.\n",
    "\n",
    "### Data Collection/Selection\n",
    "We will be loading data from a kaggle dataset. More information here: https://www.kaggle.com/datasets/thedatasith/hotdog-nothotdog and here: https://www.youtube.com/watch?v=ACmydtFDTGs\n",
    "\n",
    "The data are organized into testing and training folders with each folder containing subfolders for each class of object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15419ea2-de41-4485-a3d6-9549e057bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the URL of the zip file and the local paths\n",
    "zip_file_name = 'hotdog_nothotdog.zip'\n",
    "zip_file_url = f'https://www.thislondonhouse.com/data/{zip_file_name}'  # Replace with the URL of the zip file\n",
    "extract_to_dir = 'data/images'  # Replace with your desired extraction folder\n",
    "\n",
    "# Download the file from the URL\n",
    "response = requests.get(zip_file_url)\n",
    "with open(zip_file_name, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "print(f\"Downloaded zip file to: {zip_file_name}\")\n",
    "\n",
    "# Create the extraction directory if it doesn't exist\n",
    "os.makedirs(extract_to_dir, exist_ok=True)\n",
    "\n",
    "# Open and extract the zip file\n",
    "if os.path.exists(zip_file_name):\n",
    "    with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to_dir)\n",
    "        print(f\"Files extracted to: {extract_to_dir}\")\n",
    "\n",
    "    # Delete the file\n",
    "    os.remove(zip_file_name)\n",
    "    print(f\"{zip_file_name} has been deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71d81fc-318a-4174-b564-f24fa71c5c25",
   "metadata": {},
   "source": [
    "Though we are analyzing iamges, we will still want to profile the data. This includes exploring a subset of the data to understand what we are analyzing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3768d0-ecf5-4f4e-855b-375d48786423",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'data/images/hotdog-nothotdog/hotdog-nothotdog/test'\n",
    "train_dir = 'data/images/hotdog-nothotdog/hotdog-nothotdog/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69fb5f4-1c9e-4745-a59f-8335c5085285",
   "metadata": {},
   "source": [
    "This is a sample of 'hotdog' images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de517ab8-d491-490b-9cdb-c174b0513965",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotdog_images = load_random_images(f\"{train_dir}/hotdog\", 45)  # Adjust numbers as needed\n",
    "plot_images_grid(hotdog_images, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107eb01-faa9-4841-b028-3a4fb9d2d977",
   "metadata": {},
   "source": [
    "This is a sample of 'nothotdog' images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a32de-6bc2-4bcc-9150-fa19b17caad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotdog_images = load_random_images(f\"{train_dir}/nothotdog\", 45)  # Adjust numbers as needed\n",
    "plot_images_grid(hotdog_images, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cfdb33-97e5-4f65-8570-6dc45537344d",
   "metadata": {},
   "source": [
    "Now, we will load our testing and trainging data. Whereas previous analyses exercises subset the training and testing from the total dataset, image dataset are often presorted into folders to ease the analysis process. So, we will load our training data first and then our testing data.\n",
    "\n",
    "When loading the data, we will perform several standardization steps. These steps are similar in purpose to the cleaning steps for text analysis. In this case, we will standardize the size of the image to 256 color 128x128px image. Then we will divide the pixel values by 255 to place each pixel value on a scale of 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6ee05-654d-4f56-bb38-4e02a37e7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data_from_directory(train_dir)\n",
    "print(f\"Training data: {len(X_train)} samples; Shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd59410-f6db-4685-8ea6-4adc79a6ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = load_data_from_directory(test_dir)\n",
    "print(f\"Testing data: {len(X_test)} samples; Shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6dd78-80b0-46dd-9e3d-980e6a7e05bd",
   "metadata": {},
   "source": [
    "Now, we will visualize the data that will be fed into our analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb855e29-81ee-4380-8001-809277d93f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 36\n",
    "random_indices = random.sample(range(len(X_train)), num_images)\n",
    "random_images = [X_train[i] for i in random_indices]\n",
    "\n",
    "# Create a 3x3 grid\n",
    "fig, axes = plt.subplots(6, 6, figsize=(8, 8))\n",
    "\n",
    "# Plot each image\n",
    "for ax, img in zip(axes.flatten(), random_images):\n",
    "    ax.imshow(img)  # Display the image\n",
    "    ax.axis('off')  # Hide the axes for better aesthetics\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc8ec50-c090-4961-bba8-d268ac547dbf",
   "metadata": {},
   "source": [
    "### Model Specification\n",
    "As with the text analysis exercise, in this exercise, we will use several transformers specially designed to process image data. The first will convert all images to grayscale because shape rather than color is a more important indicator of a hotdog. This also reduces the complexity of the problem. Rather than dealing with three colors (red, blue, green) and intensities of each, we are only dealing with the intensities of one color (black). Next, we will perform a histogram of gradients fuction (HOG). This function seeks to further reduce the complexity of the image while also emphasizing important characteristics such as edges and the direction of objects. Finally, we will flatten the image so that each image is represented as a single vector of values rather than a matrix of values.  \n",
    "\n",
    "As with previous exercises, we will begin with a logitistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ff0e6-a8cc-4b05-860e-7472641d2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with the custom transformer\n",
    "pipeline = Pipeline([\n",
    "    ('grayscale', FunctionTransformer(to_grayscale, validate=False)),  # Convert images to grayscale\n",
    "    ('hog', FunctionTransformer(lambda x: extract_hog_features(x), validate=False)),  # Extract HOG features\n",
    "    ('flatten', FunctionTransformer(flatten_images, validate=False)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457626d-0049-4c1d-9d21-4f4565e62915",
   "metadata": {},
   "source": [
    "Fit the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f14b9-8c1e-442e-8f1d-9c5ff262fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd1ad1-f91e-42d0-a89f-860cf8aaf53e",
   "metadata": {},
   "source": [
    "And predict the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c2a33-16e4-4a83-9d8d-7c453d49f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_predicted = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042d2b74-fe60-42de-a4a0-186e3f74796d",
   "metadata": {},
   "source": [
    "Assess classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81280e8b-c24a-4d84-a930-55c4339db966",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_performance(y_test, logistic_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a71d0-a83d-4415-be84-38e81a74ef0c",
   "metadata": {},
   "source": [
    "For this exercie, a convolutional neural network is added to the end of the list of comparison classifiers. This type of neural netowrk is specially designed to process images and to detect objects. The following code applies early stopping rules to the neural network to limit overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777db46-d205-4033-b214-dfd29076390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f2b071-01c5-47b0-af2f-19a7b1dfbad0",
   "metadata": {},
   "source": [
    "As with the text analysis exercises, we will then run a series of classifiers to assess the quality of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd6794-7073-4477-8173-8fb87ed9e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "classifiers = ((DummyClassifier(), \"Dummy Classifier\"),\n",
    "               (LogisticRegression(C=5, max_iter=10000), \"Logistic Regression\"),\n",
    "               (RidgeClassifier(alpha=1.0, solver=\"sparse_cg\"), \"Ridge Classifier\"),\n",
    "               (KNeighborsClassifier(n_neighbors=100), \"kNN\"),\n",
    "               (RandomForestClassifier(), \"Random Forest\"),\n",
    "               (SVC(kernel='linear', C=1.0, max_iter=10000), \"Linear SVC\"),\n",
    "               (SGDClassifier(loss=\"log_loss\", alpha=1e-4, n_iter_no_change=3, early_stopping=True), \"log-loss SGD\",),\n",
    "               (NearestCentroid(), \"NearestCentroid\"),\n",
    "               (ComplementNB(alpha=0.1), \"Complement naive Bayes\"), \n",
    "               (KerasClassifier(model=create_sequential_model((128,128,3), ['accuracy']), epochs=10, batch_size=5, verbose=1, validation_split=0.2, callbacks=[early_stopping]), 'Neural Network'))\n",
    "\n",
    "for clf, name in classifiers:\n",
    "    print(\"=\" * 80)\n",
    "    print(name)\n",
    "    print(\"_\" * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    if name == 'Neural Network':\n",
    "        ## no need to flatten for neural network\n",
    "        pipeline = Pipeline([\n",
    "            ('classifier', clf) \n",
    "        ])\n",
    "    else:\n",
    "        pipeline = Pipeline([\n",
    "            ('grayscale', FunctionTransformer(to_grayscale, validate=False)),  # Convert images to grayscale\n",
    "            ('hog', FunctionTransformer(lambda x: extract_hog_features(x), validate=False)),  # Extract HOG features\n",
    "            ('flatten', FunctionTransformer(flatten_images, validate=False)),\n",
    "            ('classifier', clf) \n",
    "        ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    train_time = time() - t0\n",
    "    print(f\"train time: {train_time:.3}s\")\n",
    "\n",
    "    t0 = time()\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(f\"test time:  {test_time:.3}s\")\n",
    "    classifier_performance(y_test, y_pred, {0: 'Not Hot Dog', 1: 'Hot Dog'})\n",
    "    print()\n",
    "    if name:\n",
    "        clf_descr = str(name)\n",
    "    else:\n",
    "        clf_descr = clf.__class__.__name__\n",
    "\n",
    "    results.append((clf_descr, metrics.accuracy_score(y_test, y_pred), train_time, test_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f661d940-f5fe-4e15-a066-2d88a546c843",
   "metadata": {},
   "source": [
    "Processing images is a resource intensive task, so it will be particularly important to consider the efficiency of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc4dc9-9d62-44c9-9e13-a3960f718c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time)\n",
    "test_time = np.array(test_time)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\n",
    "ax1.scatter(score, training_time, s=60)\n",
    "ax1.set(\n",
    "    title=\"Score-training time trade-off\",\n",
    "    yscale=\"log\",\n",
    "    xlabel=\"test accuracy\",\n",
    "    ylabel=\"training time (s)\",\n",
    ")\n",
    "ax2.scatter(score, test_time, s=60)\n",
    "ax2.set(\n",
    "    title=\"Score-test time trade-off\",\n",
    "    yscale=\"log\",\n",
    "    xlabel=\"test accuracy\",\n",
    "    ylabel=\"test time (s)\",\n",
    ")\n",
    "\n",
    "for i, txt in enumerate(clf_names):\n",
    "    ax1.annotate(txt, (score[i], training_time[i]))\n",
    "    ax2.annotate(txt, (score[i], test_time[i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29854c5-0eba-423f-bec0-dd27630468c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = get_image('https://thislondonhouse.com/Images/hotdog.jpg')\n",
    "plt.imshow(test_image[0])\n",
    "plt.show()\n",
    "pipeline.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c1c7a-5f7b-4f7e-894d-df77c7b3d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = get_image('https://thislondonhouse.com/Images/tacos.jpg')\n",
    "plt.imshow(test_image[0])\n",
    "plt.show()\n",
    "pipeline.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd19927-0f75-4e6d-a393-1534cadac40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = get_image('https://thislondonhouse.com/Images/puppy.jpg')\n",
    "plt.imshow(test_image[0])\n",
    "plt.show()\n",
    "pipeline.predict(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33abdb6-d1cf-4783-8ce5-ae7ada04ecc8",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The model performed moderately well. It did better than the dummy classifier which predicts the same class regardless, but it was not much better than a coin flip. There could be several reasons for this. The training sample may not be large enough. It may be necessary to simply take more pictures of hot dogs. Alternatively, we could introduce synthetic data because the orientation of a hot dog is not a distinguishing feature.  Also, hot dogs are fairly common food items, but they may often be pictured on a plate. This would suggest to the classifier that the plate is part of the hot dog, increasingly the likelihood that any other food item would be classified as a hot dog. The risk of misclassifying a hot dog is very low so the cost of implementing our classifier, is low. If the task was more existential such as predicting illness, our classifier would be insufficient to the task and may do more harm than good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde1a9a-8fe8-4ff4-9831-337f6e412aa7",
   "metadata": {},
   "source": [
    "## Image Exercise 2\n",
    "In this exerise, we will be building an LLM-wrapper application. These steps will serve as a model for how we approach LLM-wrappers in the future.  \n",
    "\n",
    "### Business Problem\n",
    "Creating copy for online stores is a labor intensive process. Though very few people actually read the copy, web crawlers do and product descriptions are essential for developing an effective search engine optimization (SEO) strategy. Small companies often lack the resources needed to develop high quality product descriptions in an efficient manner. So, it would be valuable to have an LLM that can 'see' our products and describe them in a way that fits our store's identity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea2dfb-3b04-46d4-8d4b-a2ea48880a87",
   "metadata": {},
   "source": [
    "### Data Collection/Selection\n",
    "For this exercise, I have downloaded an image from an Etsy store: https://www.etsy.com/listing/671179169/linen-dress-long-midcalf-belt-dress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d4269-809e-41ea-a3c4-b78fc04019c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://i.etsystatic.com/7803582/r/il/7f4b28/2007580738/il_1140xN.2007580738_1ttb.jpg\"\n",
    "\n",
    "Image.open(BytesIO(requests.get(image_url).content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c266eb50-f743-4785-aaae-33fb3f8ba182",
   "metadata": {},
   "source": [
    "### LLM Engineering\n",
    "In this exercise, the LLM is our intelligence, but we have to tell it what kind of intelligence to exhibit. The unlike the chat features which allow us to specify the system behavior, we must embed all instructions in a single prompt. The image is then passed and the LLM assesses the image based on the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ac06d-bba6-4408-96d1-44413d6bc0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "    What do you see?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47dde06-f3e2-4a1e-ab8b-b50fc37bca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "    You are an assistant at an upscale boutique and you need to describe our new line of spring dresses. \n",
    "    Could you describe this dress in a way that would be enticing to upscale customers who refresh their wardrobe annually?\n",
    "    Do not provide any another commentary. Only describe the dress.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b78fb-b2dd-4291-a0fe-dcc42b25341f",
   "metadata": {},
   "source": [
    "### Application Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b25c5-0ae6-4d95-80f9-f740d06a2443",
   "metadata": {},
   "source": [
    "Again, we will be using Groq for LLM inference. [sign up for API access](https://console.groq.com/login). We will use a free level of service, but there are paid levels. So it is important to protect your key. Once you have created an API key, you can add it as a variable to a variables.env file to obscure the key from your source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac534c9-8c40-425e-9429-e2dbc92e1f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = 'variables.env'\n",
    "\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041a35c-200b-4799-9a63-8ea976bd9297",
   "metadata": {},
   "source": [
    "Here we load the environment variable from the variables.env file and pass it into the Groq library to establish a link to their inference resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355280e2-fa7e-49fa-8a03-7e3f5e897c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7606c905-cd33-49d1-9fc0-eef3e4922557",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": instruction\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(image_url)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfc7f07-55c5-4d95-86bc-87b837ec118b",
   "metadata": {},
   "source": [
    "## Image Exercise 3\n",
    "In this exerise, we will be building an LLM-wrapper application. These steps will serve as a model for how we approach LLM-wrappers in the future.  \n",
    "\n",
    "### Business Problem\n",
    "Many organizations are still heavily reliant on paper. It takes time, effort, and money to transform paper processes to digital processes and the transformation is often slow and uneven. Therefore, it would be useful to have tools that can read paper documents and accurately transcribe the information into digital systems. In this exercise, we will have the LLM read a change of minor form and extract pertinent information from it.\n",
    "\n",
    "### Data Collection/Selection\n",
    "For this exercise, I have scanned a change of minor form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2156ee3f-201c-4d86-9a69-c974be6c0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/minor_drop.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d421743-57d0-447f-8777-7bd53fd96ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603ed0d-9b15-498b-9e61-69986d3ee19e",
   "metadata": {},
   "source": [
    "### LLM Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce3c77-5534-42da-ac26-c75471c436e4",
   "metadata": {},
   "source": [
    "Here I provide instructions for how I want the LLM to assess the image and how I want it to respond. The goal of this exercise would be to extract the information from the form and then load it into some subsequent digital system. Therefore, we will ask the LLM to format the data in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc9659-df3f-40d0-8602-b00798f61bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "    The following is an add/drop form for Loyola University Maryland.\n",
    "    The form contains data entry boxes for Student ID, Current Major, Student Athlete, Class Year, Last Name, First Name, M.I..\n",
    "    Can you read the form and find this information?\n",
    "    Report this information in JSON format.\n",
    "    Only extract this information in JSON. Do not provide any other commentary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63265342-c628-4a6e-9dc9-afa1294c09f1",
   "metadata": {},
   "source": [
    "### Application Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a447ecd-3913-4c6b-8b53-6fedd078b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\", \n",
    "                     \"text\": instruction\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/png;base64,{encode_image(image_path)}\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
