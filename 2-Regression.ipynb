{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ecc1039-fd20-4250-99f3-606c1fc4083f",
   "metadata": {},
   "source": [
    "# Module 02 - Regression\n",
    "## Feature Selection\n",
    "Feature selection is a critical step in machine learning that involves identifying the most relevant features for a given model. By carefully selecting features, we can improve the model's performance, reduce overfitting, and enhance interpretability. Including irrelevant or redundant features can lead to increased computational complexity and poorer generalization to new data. Moreover, a well-curated feature set can provide valuable insights into the underlying data patterns and relationships, enabling more informed decisions.\n",
    "\n",
    "There are several techniques for feature selection, each with its strengths and applications. Methods like correlation analysis and Variance Inflation Factor (VIF) can help identify and remove highly correlated features. Tree-based models such as Random Forests and Gradient Boosted Trees can provide feature importance scores, highlighting the most impactful features. Additionally, techniques like Recursive Feature Elimination (RFE) systematically eliminate less important features, while dimensionality reduction methods such as Principal Component Analysis (PCA) transform features into uncorrelated components. Combining these techniques can create a robust feature selection strategy, ultimately leading to better-performing and more efficient machine learning models.\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "Hyperparameter tuning is an essential process in machine learning that involves optimizing the hyperparameters of a model to achieve the best possible performance. Unlike model parameters, which are learned from the data during training, hyperparameters are set before the training process and control the behavior of the model. Properly tuned hyperparameters can significantly improve the model's accuracy, prevent overfitting, and ensure that the model generalizes well to new, unseen data. The process of hyperparameter tuning involves selecting the best combination of hyperparameters that maximize the model's performance based on a specific evaluation metric.\n",
    "\n",
    "There are several techniques for hyperparameter tuning, each with its own advantages. Grid Search is a popular method that involves an exhaustive search over a specified parameter grid, evaluating each combination to find the best one. Random Search, on the other hand, randomly samples from the hyperparameter space and can be more efficient when dealing with a large number of hyperparameters. Advanced techniques such as Bayesian Optimization and Hyperband offer more sophisticated approaches by modeling the performance of hyperparameters and dynamically allocating resources to promising configurations. Cross-validation is often used in conjunction with these techniques to ensure that the selected hyperparameters lead to robust and reliable models. By employing these strategies, machine learning practitioners can fine-tune their models to achieve optimal performance and make data-driven decisions with confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9205cb-a976-4236-8097-394255db8032",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Regression analysis is a fundamental statistical technique in machine learning that seeks to understand and predict the relationship between a dependent variable (the outcome being predicted) and one or more independent variables (the predictors). At its core, regression aims to model how changes in input features correspond to changes in the target variable, allowing practitioners to make quantitative predictions and understand the underlying patterns in data. Unlike classification, which predicts discrete categories, regression focuses on predicting continuous numerical values, making it crucial for tasks that involve forecasting, estimation, and understanding complex relationships across various fields like economics, finance, scientific research, and machine learning.\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"https://thislondonhouse.com/Jupyter/Images/regression-linefit.png\"></p>\n",
    "\n",
    "The fundamental goal of regression is to find the most appropriate mathematical function that best describes the relationship between variables, minimizing the difference between predicted and actual values. This process involves selecting an appropriate regression model based on the data's characteristics, such as linearity, complexity, and potential non-linear relationships. Regression techniques range from simple methods like linear regression, which assumes a straight-line relationship, to more complex approaches like neural network and gradient boosting regression that can capture intricate, non-linear patterns. By carefully choosing and tuning regression models, data scientists can develop predictive tools that not only forecast outcomes but also provide insights into the underlying mechanisms driving those predictions.\n",
    "\n",
    "### Regression Algorithms\n",
    "#### Linear Regression\n",
    "Linear regression serves as the fundamental building block of regression analysis, establishing a direct linear relationship between independent variables (features) and a dependent variable (target). At its core, it aims to find a linear function that best describes this relationship by fitting a line (in 2D) or hyperplane (in higher dimensions) to the data points. The model determines the optimal coefficients (weights) for each feature by minimizing the sum of squared differences between predicted and actual values, known as the least squares method. While simple and interpretable, linear regression makes several key assumptions: linearity in parameters, independence of observations, homoscedasticity (constant variance in errors), and normally distributed errors. Despite these limitations, it remains a powerful tool for understanding relationships between variables and making predictions when these assumptions are reasonably met. The main advantage of linear regression is its simplicity and interpretability, making it excellent for baseline models and understanding feature importance, but its strict linearity assumption means it cannot capture complex non-linear relationships in data.\n",
    "#### Ridge and Lasso Regression\n",
    "Ridge regression, also known as L2 regularization or Tikhonov regularization, addresses some of the key limitations of standard linear regression, particularly when dealing with multicollinearity or overfitting. It modifies the linear regression cost function by adding a penalty term proportional to the sum of squared coefficient values, multiplied by a regularization parameter λ (lambda). This penalty term effectively shrinks the coefficients toward zero but never exactly to zero, creating a more stable model by reducing the impact of correlated features. Lasso regression (Least Absolute Shrinkage and Selection Operator) introduces a different approach to regularization by adding an L1 penalty term to the linear regression cost function. Unlike ridge regression, lasso uses the absolute values of coefficients in its penalty term, which can force some coefficients exactly to zero, effectively performing automatic feature selection. Ridge and Lasso regressions are particularly valuable when dealing with many features or when features are highly correlated, as it helps prevent the extreme coefficient values that often occur in these situations. \n",
    "#### Support Vector Regression\n",
    "Support Vector Regression (SVR) adapts the principles of Support Vector Machines to regression problems, offering a powerful approach for handling both linear and non-linear relationships. Instead of fitting a line to minimize squared errors, SVR attempts to find a function that deviates from the observed values by no more than a specified margin ε (epsilon) while remaining as flat as possible. It employs a flexible \"tube\" of width 2ε around the function, only penalizing predictions that fall outside this tube. One of SVR's key strengths is its ability to handle non-linear relationships through the kernel trick, which implicitly maps the input features to a higher-dimensional space where a linear relationship might better represent the data. Common kernel choices include polynomial, radial basis function (RBF), and sigmoid functions, making SVR highly adaptable to different types of relationships in the data. SVR's major advantage is its robustness to outliers and ability to handle non-linear relationships effectively, but its main disadvantage is the computational complexity that makes it less suitable for very large datasets.\n",
    "#### Gradient Boosting Regression\n",
    "Gradient Boosting Regression represents an advanced ensemble learning technique that sequentially builds regression models to create a powerful predictive framework. Unlike traditional regression methods, gradient boosting constructs a series of weak learners (typically decision trees) where each subsequent model focuses on correcting the errors of the previous models, effectively \"boosting\" the overall predictive performance. The method works by iteratively adding new models that predict the residual errors of the existing ensemble, with each new model trained to minimize the remaining prediction error through gradient descent optimization. This approach allows gradient boosting to capture complex, non-linear relationships in the data while maintaining relatively good interpretability compared to neural networks. The core strength of gradient boosting lies in its ability to handle diverse data types, reduce overfitting through techniques like regularization and tree pruning, and often achieve state-of-the-art predictive performance across various domains. Despite the advantages, it can be computationally intensive and requires careful hyperparameter tuning to prevent overfitting, especially on smaller datasets.\n",
    "#### Neural Network Regression\n",
    "Neural network regression uses artificial neural networks to model complex, non-linear relationships between inputs and outputs. Unlike traditional regression methods, neural networks can automatically learn hierarchical representations of features through multiple layers of interconnected neurons, each applying non-linear activation functions to weighted combinations of inputs. This architecture allows neural networks to capture intricate patterns and interactions that simpler models might miss. The training process uses backpropagation to adjust the network's weights and biases, minimizing a loss function that measures the difference between predicted and actual values. Neural network regression can be customized through various architectural choices, including the number of layers, neurons per layer, activation functions, and regularization techniques like dropout. While potentially more powerful than simpler regression methods, neural networks typically require larger datasets for training, more computational resources, and careful tuning of hyperparameters to achieve optimal performance. The primary advantage of neural networks is their unparalleled ability to capture complex patterns in data, but this comes at the cost of reduced interpretability and the need for substantial computational resources and data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ebe7e9-4aa6-4958-b6b5-b9a129d9dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pybaseball as pb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962fa551-4e3a-42cf-9e32-b40c1c3b2386",
   "metadata": {},
   "source": [
    "## Regression Exercise 1\n",
    "The following functions are helper functions for this exercise. The first simply removes punctuation from a string. We will use it to format player's names so that there are no special characters that may make name-matching more difficult. The second function builds our dataset using the [pybaseball](https://github.com/jldbc/pybaseball) library. This function downloads batter data from 2018 to the present and pairs that data with player profile data, which includes salary. The third function provides a formatted summary of regression results. We will use that to compare performance across algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead479f-9eef-4e1c-88c8-1afdb61d3b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def remove_punctuation(name):\n",
    "    return name.translate(str.maketrans('', '', ' ' + string.punctuation)).lower()\n",
    "\n",
    "def create_batter_data():\n",
    "    batter_data = pb.batting_stats(2018, 2024, qual=50)\n",
    "    player_data = pb.bwar_bat()\n",
    "\n",
    "    batter_data['key'] = batter_data['Name'].apply(remove_punctuation) + batter_data['Season'].astype(str)\n",
    "    player_data['key'] = player_data['name_common'].apply(remove_punctuation) + player_data['year_ID'].astype(str)\n",
    "\n",
    "    player_data = player_data.sort_values(by=['mlb_ID', 'year_ID'])\n",
    "\n",
    "    # Create a new column 'Previous_Year_Salary' with the previous year's salary for each employee\n",
    "    player_data['prev_salary'] = player_data.groupby('mlb_ID')['salary'].shift(1)\n",
    "    player_data = player_data.dropna()\n",
    "\n",
    "    batter_data = batter_data.merge(player_data[['key', 'lg_ID', 'prev_salary', 'salary']], on='key', how=\"inner\")\n",
    "\n",
    "    for column in batter_data.columns.tolist():\n",
    "        if batter_data[column].isnull().sum() > 0:\n",
    "            batter_data = batter_data.drop(column, axis=1)\n",
    "\n",
    "    batter_data.to_csv(\"data/batter_data.csv\", index=False)\n",
    "    return batter_data\n",
    "\n",
    "def regression_preformance(y_true, y_pred, importance_df=None):\n",
    "    mse = metrics.mean_squared_error(y_true, y_pred)\n",
    "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f'''\n",
    "Mean Squared Error (MSE): {mse:.4f}\n",
    "Mean Absolute Error (MAE): {mae:.4f}\n",
    "R-squared (R2): {r2:.4f}\n",
    "''')\n",
    "\n",
    "    # Plotting true vs predicted values\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    ax1.scatter(y_pred, y_true)\n",
    "    ax1.plot(y_true, y_true, color='red')\n",
    "    # ax1.ticklabel_format(style='plain', axis='y')\n",
    "    # ax1.ticklabel_format(style='plain', axis='x')\n",
    "    ax1.set_ylabel('True Values')\n",
    "    ax1.set_xlabel('Predicted Values')\n",
    "    ax1.set_title('True vs Predicted Values')\n",
    "\n",
    "    if importance_df is not None:\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df.sort_values(by='Importance', ascending=False), ax=ax2)\n",
    "        ax2.set_title('Feature Importance')\n",
    "        ax2.set_xlabel('Importance')\n",
    "        ax2.set_ylabel('Feature')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e2c1d-d717-452f-a3ad-d7675f31da25",
   "metadata": {},
   "source": [
    "### Business Problem\n",
    "Baseball is a highly competetive sport among players and team owners. Owners compete by spending money to acquire the best players. Ownership rules in baseball are structured in such a way as to allow almost limitless access to the best players. Whereas other professional sports leagues may have hard salary caps to increase parity and competition, baseball has a luxury tax which penalizes owners who spend 'too much' on their team. Part of this tax is distributed to others members of major league baseball. So, there is a great need to understand the relationship between salary and on-field performance.\n",
    "\n",
    "### Data Collection\n",
    "This code tries to load the batter_data.csv file from the data folder. If the attempt fails (because the file is not found), it builds the file for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51e255-8e52-421c-a2ee-0efb867c16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    batter_df = pd.read_csv('data/batter_data.csv')\n",
    "except FileNotFoundError:\n",
    "    batter_df = create_batter_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132478c-e7b6-422d-884f-79408c775cdf",
   "metadata": {},
   "source": [
    "Data are orgnized in tabular format with each record representing an individual player in a given year. The target variable is 'salary' which represents the player's salary for a given year. The data contain over 200 variables. \n",
    "\n",
    "The following line will load the data as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69e93d5-bb2e-4d8a-825d-479e5f2b39ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df = pd.read_csv(\"data/batter_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ebd8f-04e0-4efb-9752-c7748b4fb6fe",
   "metadata": {},
   "source": [
    "### Data Profiling\n",
    "Once the data are loaded, we need to profile the data and prepare it for analysis. This typically involves several steps that may include handling missing data, exploring data, feature selection, among others. The steps will vary depending on the dataset and the business problem, but profiling always precedes model building.  \n",
    "\n",
    "The following lines provide important insight into the nature of the data. As you can see there are dozens of variables that may be important when predicting salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7bddd-b0ae-42b7-8d3d-ab58a9a1e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(salary_df.info(verbose=True, show_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96cbd7-0d4f-439d-b589-11e2e6ab1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(salary_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4169fe9f-bc99-4e23-b48e-60bde9fdfcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(salary_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2e55b-9b8d-46a6-8812-f13201afdd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df.hist(figsize=(12, 10), bins=30, edgecolor=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d1c18-c659-4ef5-8a30-41570cfca117",
   "metadata": {},
   "source": [
    "It is almost impossible to comprehend this much data. So, it is important that we start with a subset of the data and then iteratively build a model that helps us understand the relationship between pay and performance. Thus, the following lines are important as they provide an intial theory for how players are evaluated: **On-field individual performance determines pay**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1e6ae-7ba9-4c87-af13-f47936365f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['salary']\n",
    "categorical_cols = ['lg_ID']\n",
    "numeric_cols = ['AVG','prev_salary']\n",
    "count_cols = ['Age', 'PA', '1B', '2B', '3B', 'HR', 'RBI', 'BB', 'SO', 'SF', 'SH', 'SB', 'CS', 'Pitches', 'Balls', 'Strikes']\n",
    "input_cols = [x for x in categorical_cols + numeric_cols + count_cols if x not in target_cols]\n",
    "data_cols = input_cols + target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa934bf6-13f8-4638-ace0-843da76b6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = salary_df[data_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a89ff-5365-4c2a-a422-c67c7a730ed3",
   "metadata": {},
   "source": [
    "This is a more manageable subset of the data. We can see that we have 17 player performance metrics, a variable representing the league (National or American) and their previous salary (this allows us to 'control' for previous salary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2a16e-cd66-4011-ab5b-f159ffac84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info(verbose=True, show_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa2bcc-b3ca-48d0-89de-1a949755a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cce82c-3292-4466-9ac4-c4ce6e017738",
   "metadata": {},
   "source": [
    "The following illustrates that most of our data are count data and will need to be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b01bf-ce79-4328-8dea-73d0ba4c46fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[input_cols].hist(figsize=(12, 10), bins=30, edgecolor=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb62ead9-39c2-46ef-8a11-f34a6b61afac",
   "metadata": {},
   "source": [
    "The following chart illustrates the correlation among variables. Hot (i.e., white) values indicate high levels of correlation. Too much coorelation among variables may cause the models to fail or to have poor prediction performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5127811e-ae6d-484f-872e-196f927c5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the heatmap\n",
    "sns.heatmap(df.select_dtypes('number').corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871d25f-74dd-468b-a0e8-f66b14d7bd12",
   "metadata": {},
   "source": [
    "Here, you can see that salaries range from 100k to 40 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9a377-6ddd-4ca0-9b89-299854f5666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[target_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4842e1-9f3e-4f87-8648-4e3089bbfb47",
   "metadata": {},
   "source": [
    "Next, we will drop rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14f538-cebe-4c3e-a44c-005b2bc891a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0420f0-efd6-4c0d-861d-7fd4bdd07dc6",
   "metadata": {},
   "source": [
    "Finally, we will create our testing and training samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9cd42-491a-4202-9c38-52de174f8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[input_cols], df[target_cols], test_size=0.25, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51edabdf-3368-4061-a6f0-34f9a46c60c1",
   "metadata": {},
   "source": [
    "### Model Specification\n",
    "#### Preprocessing\n",
    "As with the previous exercises, we will create some standard transformers to handle different kinds of data. Categorical variables must be transformed into number, count data must be log-transformed and scaled, and numeric data must be scaled. These pipelines ensure consistency. We may not always need all pipelines, but we will always want to handle these datatypes in a consistent way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b1852-ac20-40f4-a728-e22ae293636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "count_transformer = Pipeline(steps=[\n",
    "    ('log', FunctionTransformer(np.log1p)),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f0252-8c57-4049-a547-7861a9e5861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc8341-9262-4060-8675-261ecb3c86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f50a728-b3e7-439d-8f3e-7c0cb2fb7516",
   "metadata": {},
   "source": [
    "#### Model Selection\n",
    "We will use the same preprocessor for all of the following models. This may not always be the case as we will see in subsequent models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c7bb0-5166-472a-b9e4-74fe4ef84431",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('count', numeric_transformer, count_cols),\n",
    "        ('cont', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc8bf9-9371-4d32-abca-36a4fe1ab252",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LinearRegression() # Linear Regression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "]) # Define the pipeline\n",
    "\n",
    "pipeline.fit(X_train, np.ravel(y_train)) # Fit the pipeline\n",
    "\n",
    "y_predicted = pipeline.predict(X_test) # Use model to predict test data\n",
    "\n",
    "print(\"Linear Regression Performance Metrics:\")\n",
    "regression_preformance(y_test, y_predicted, pd.DataFrame({'Feature': preprocessor.get_feature_names_out(), 'Importance': base_model.coef_})) # Assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a0d56-c391-4496-b7ed-7d4fb296ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DummyRegressor(strategy=\"mean\") # Dummy Regressor\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "]) # Define the pipeline\n",
    "\n",
    "pipeline.fit(X_train, np.ravel(y_train)) # Fit the pipeline\n",
    "\n",
    "y_predicted = pipeline.predict(X_test) # Use model to predict test data\n",
    "\n",
    "print(\"Dummy Regression Performance Metrics:\")\n",
    "regression_preformance(y_test, y_predicted) # Assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b735ae-3014-4d8b-bf10-a5270197de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Ridge(random_state=42) # Ridge Regression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "]) # Define the pipeline\n",
    "\n",
    "pipeline.fit(X_train, np.ravel(y_train)) # Fit the pipeline\n",
    "\n",
    "y_predicted = pipeline.predict(X_test) # Use model to predict test data\n",
    "\n",
    "print(\"Ridge Regression Performance Metrics:\")\n",
    "regression_preformance(y_test, y_predicted, pd.DataFrame({'Feature': preprocessor.get_feature_names_out(), 'Importance': base_model.coef_})) # Assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095bad9-caa1-46dc-a4f1-a9dfd0914b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = SVR(kernel='linear') # Support Vector Machines Regression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "]) # Define the pipeline\n",
    "\n",
    "pipeline.fit(X_train, np.ravel(y_train)) # Fit the pipeline\n",
    "\n",
    "y_predicted = pipeline.predict(X_test) # Use model to predict test data\n",
    "\n",
    "print(\"SVM Regression Performance Metrics:\")\n",
    "regression_preformance(y_test, y_predicted, pd.DataFrame({'Feature': preprocessor.get_feature_names_out(), 'Importance': base_model.coef_[0]})) # Assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed40a3d-cfaf-4807-8624-e0a1e4fad0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = GradientBoostingRegressor(random_state=42) # Gradient Boosting Regressor\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "]) # Define the pipeline\n",
    "\n",
    "pipeline.fit(X_train, np.ravel(y_train)) # Fit the pipeline\n",
    "\n",
    "y_predicted = pipeline.predict(X_test) # Use model to predict test data\n",
    "\n",
    "print(\"Gradient Boosting Regression Performance Metrics:\")\n",
    "regression_preformance(y_test, y_predicted, pd.DataFrame({'Feature': preprocessor.get_feature_names_out(), 'Importance': base_model.feature_importances_})) # Assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32842bed-7ccd-4b9b-996d-5506013dae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = XGBRegressor(enable_categorical=True) # XBBoost Regressor\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "]) # Define the pipeline\n",
    "\n",
    "pipeline.fit(X_train, np.ravel(y_train)) # Fit the pipeline\n",
    "\n",
    "y_predicted = pipeline.predict(X_test) # Use model to predict test data\n",
    "\n",
    "print(\"XGBoost Regression Performance Metrics:\")\n",
    "regression_preformance(y_test, y_predicted, pd.DataFrame({'Feature': preprocessor.get_feature_names_out(), 'Importance': base_model.feature_importances_})) # Assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08dce5-ff2e-4499-8c51-1731d1d6cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dims): # Define the structure of the model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(dims,)))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "base_model = KerasRegressor(create_model(preprocessor.fit_transform(X_train).shape[1]), epochs=100, batch_size=10, verbose=1) # Neural Network Regressor\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "]) # Define the pipeline\n",
    "\n",
    "pipeline.fit(X_train, np.ravel(y_train)) # Fit the pipeline\n",
    "\n",
    "y_predicted = pipeline.predict(X_test) # Use model to predict test data\n",
    "\n",
    "print(\"Neural Network Regression Performance Metrics:\")\n",
    "regression_preformance(y_test, y_predicted) # Assess model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ea29d-f4d7-4cac-ac2f-358891e5403f",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "Model evaluation involves more than just questions of whether the model performed well. It also includes questions of whether this is the correct model or did we use the correct parameters when building a model. As you can see, the SVM regression performed poorly. It may be because our data are bad, or it may be because the algorithm was not tuned correctly. \n",
    "#### Hyperparameter Tuning: Grid Search Cross-Validation\n",
    "To check the latter, we can use a techique known as hyperparameter tuning. This method uses a comprehensive grid search of potential hyperparameter values, testing the model with each permutation of parameter values and cross-validating each test 5 times. This will help us identify the hyperparameters that best fit our data and the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01617b6-43f3-4ad6-8aaf-4150dd6f3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = SVR()\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__C': [0.1, 1, 10, 100, 1000, 10000, 100000, 1000000],\n",
    "    'model__epsilon': [0.01, 0.1, 1, 10],\n",
    "    'model__kernel': ['linear', 'rbf'],\n",
    "    'model__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error', verbose=5)\n",
    "grid_search.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "\n",
    "y_predicted = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\"Grid Search SVM Regression Performance Metrics:\")\n",
    "regression_preformance(y_test, y_predicted, pd.DataFrame({\n",
    "    'Feature': preprocessor.get_feature_names_out(),\n",
    "    'Importance': grid_search.best_estimator_['model'].coef_[0]\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82935f35-69d8-4444-a20b-6f49505a3e46",
   "metadata": {},
   "source": [
    "As you can see, with the correct hyperparameters, the SVM regression performs in-line or better than other regression algorithms. In this case, A C value of 10000, an epison of 10, a gamma of scale, and a kernel of linear woud recreate the best result.\n",
    "\n",
    "#### Feature Selection: Recursive Feature Elimination\n",
    "In addition to hyperparameter tuning, you may also face issues selecting the appropritate features. We started with a theory about how baseball players are evaluated--**individual performance determines pay**--but this may not be a correct theory of the relationship between pay and performance. While we could offer a competing theory, we could also mine the data for a theory. Data mining seeks to find relationships among variables, independent of the meaning of those variables.\n",
    "\n",
    "Recursive feature elimination is a technique for mining data that iteratively removes features until an optimal number of features is reached. This technique is agnostic about the meaning of a variable and is only concerned with the question: **does this feature contribute to our ability to predit the target**?\n",
    "\n",
    "We start by reselecting our features, but this time we will select all numeric features and iteratively remove non-performing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebe40c-c220-4396-9dbc-32283b761d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['salary']\n",
    "categorical_cols = ['lg_ID']\n",
    "numeric_cols = [x for x in salary_df.select_dtypes(['int', 'float']).columns.tolist() if x not in target_cols]\n",
    "count_cols = []\n",
    "input_cols = [x for x in categorical_cols + numeric_cols + count_cols if x not in target_cols]\n",
    "data_cols = input_cols + target_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8b04a-f22a-4543-9475-a0ddb16f0b99",
   "metadata": {},
   "source": [
    "Once the features are selected, we will subset our data, drop missing values and create our testing/training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92127a8b-ce61-4ae2-8906-d3a12db4ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = salary_df[data_cols]\n",
    "df = df.dropna()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[input_cols], df[target_cols], test_size=0.25, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbabe55-420a-4203-8a0b-d772b57bf5d9",
   "metadata": {},
   "source": [
    "Next, we will define our preprocessor. In this case, we will not need a count transformer because we have defined all features as numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ab73a5-4ca7-4ed8-b9d8-3eda1c8c98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cont', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e416c-1595-4ece-83cc-b52bdcd7a5fe",
   "metadata": {},
   "source": [
    "Our best performing model from above is the Gradient Boosting Regressor so we will use that in our feature selection. We could have used any model, but it is generally a good idea to start with a model that you believe will perform well. \n",
    "\n",
    "Note that we are adding a Recursive Feature Elimination step to our pipeline. So, in the following example, we will transform our data, select the best features (top 40), and then feed those features into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4774c43c-c911-4575-b2ae-e6ff1988698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LinearRegression() # Linear Regression\n",
    "rfe = RFE(estimator=base_model, n_features_to_select=40, verbose=10) # Recursive Feature Elimination\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', rfe),\n",
    "    ('model', base_model)\n",
    "]) # Define the pipeline\n",
    "\n",
    "pipeline.fit(X_train, np.ravel(y_train)) # Fit the pipeline\n",
    "\n",
    "y_predicted = pipeline.predict(X_test) # Use model to predict test data\n",
    "\n",
    "print(\"Linear Regression Performance Metrics:\")\n",
    "regression_preformance(y_test, y_predicted, pd.DataFrame({'Feature': preprocessor.get_feature_names_out()[rfe.support_], 'Importance': base_model.coef_})) # Assess model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a9c4cd-043c-46e4-817d-bd54e5daa6f3",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Overall, we were able to develop a reasonable model for predicting salaries of MLB batters. Whether building a model from theory or mining for the most important factors, we were able to explain approximately 78% of the variance in hitter salaries. Such a model could be useful in identifying undervalued players, predicting negotiations among rising stars or building an economical line-up.\n",
    "\n",
    "One challenge highlighted in model specification is that the players previous salary is highly predictive of current salary. Though this is clearly true in practice, it may have a biasing effect on the salaries of rising stars who typically have lower salaries. In such cases, our model would likely predict that their future salary would be lower (perhaps much lower) than the market would be willing to pay. Missing on the low side may lead to a lower-than-is-appropriate first offer in future salary negotiations, potentially insulting the player or leading the player to explore more options. The RFE model was able to achieve similar levels of prediction without relying heavily on previous salary information, and therefore may be a preferable model when valuing high-performing but low-paid players.\n",
    "\n",
    "## Regression Exercise 2\n",
    "### Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7395eb1f-8f80-466d-be31-ac3a9793ab8e",
   "metadata": {},
   "source": [
    "Explain the business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b656185-ac7b-4ded-befc-eaa7adc9e666",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc6184d-0c24-43ae-bf15-ef53766ddbd8",
   "metadata": {},
   "source": [
    "Load the data as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be10fc76-af6e-46b5-abf2-388e3152f832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "978bc6e7-eddf-4bc7-a424-7690c10250f0",
   "metadata": {},
   "source": [
    "### Data Profiling\n",
    "Profile the data and prepare it for analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c85274-c74d-43f2-835f-1f645c5bd224",
   "metadata": {},
   "source": [
    "Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582b8eb-b32d-451e-8f8f-3833c0a36dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d63b9a66-2d96-4352-9c02-35640901a328",
   "metadata": {},
   "source": [
    "Select the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d71bf5-b208-4bf3-a87e-607827b35d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4440b9e8-c0ca-4a49-b50f-69932077612a",
   "metadata": {},
   "source": [
    "Subset features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786be61d-5962-453e-9eb2-bffe854c519d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8810751a-a052-4f3b-9d16-9da6a5439a3b",
   "metadata": {},
   "source": [
    "Explore focal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01530ef4-b666-4252-9bbc-3854761e2967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ce0605c-fc28-444c-a836-5186e1eea2d4",
   "metadata": {},
   "source": [
    "Drop rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63001e84-4b6f-4697-a1fe-21f83c3e0157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d3e1668-5d9d-4d7f-8e31-965b728668ff",
   "metadata": {},
   "source": [
    "Create our testing and training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8587ead-5dd8-4b0d-8eba-2ec36bc70f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5ebc626-59eb-4a7e-a173-109ac0235b9e",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147f5cc-4b72-418f-8145-37c2a2e9d748",
   "metadata": {},
   "source": [
    "Define preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c7bc5-9b7e-477a-9da9-5eeef628e549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c877724-44fb-4d56-8c53-78071a447c7e",
   "metadata": {},
   "source": [
    "Select model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39652062-a35a-4634-b3a8-8f3d25b2937b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f18677d3-7094-4c92-b2b6-4aec24c71875",
   "metadata": {},
   "source": [
    "Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa43ee-7f5d-49d0-a0a5-12ed07f987d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68d141a2-af89-4a0e-8f06-ab6aabea043c",
   "metadata": {},
   "source": [
    "Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524491d-a741-4d0d-9a10-7285353b8790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a28c04f5-5956-4ed7-b63b-d6dddcc23b7f",
   "metadata": {},
   "source": [
    "Predict testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0eb86-de7b-4a8a-a3ed-abde3ae9a5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e7504e4-c348-42fd-9265-6ff1a65c4d5d",
   "metadata": {},
   "source": [
    "Report performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca9991-1c05-4905-b482-4457c2570f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe824b16-52c9-42c7-b251-cd5c4dde62ac",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c47fdf3-cde3-431f-ba57-b1c965ad21d4",
   "metadata": {},
   "source": [
    "Consider feature importance/selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350596c-112b-45fe-bfd5-b6993be410d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a24feff-d4ad-4094-9a51-7faac928e566",
   "metadata": {},
   "source": [
    "Consider hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6bf558-dfb4-4fc4-bd50-8f8699674bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e80f8ccd-41fe-40a7-88f8-a25a1896990b",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da755d9-3362-4d3c-b053-a68c567eb1ab",
   "metadata": {},
   "source": [
    "Assess the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
