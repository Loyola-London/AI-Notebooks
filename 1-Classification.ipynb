{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d03c43-77e7-41fd-96ad-c5cdecc21dd9",
   "metadata": {},
   "source": [
    "# Module 1 - Classification\n",
    "## Machine Learning Basics\n",
    "Machine learning is a branch of statistics/computer science/math that is concerned with the ways in which algorithms can be trained by data and iteratively tuned (self-tuned or by human input) to perform some task (typically predicting some outcome). One way of understanding machine learning algorithms is by considering the outcome that the algorithm seeks to predict. If the outcome is a group (e.g., out/not out or malignant/benign or coupe/sedan/suv/van/truck) then the algorithm is a classifier. If the outcome is a numeric value (e.g., stock price, retention), the algorithm is a regressor.   \n",
    "\n",
    "Regardless of the type of algorithm, they all need data that will teach the algorithm how to detect the appropriate outcome. This type of data is referred to as training data. To be effective, training data must perfectly mimic real-world data, otherwise, the model will be inaccurate at best and broken at worst when used to predict real-world outcomes. \n",
    "There are three types of training techniques: supervised learning, semi-supervised learning, unsupervised learning.  \n",
    "\n",
    "1. Supervised Learning: Supervised learning refers to model training that uses labeled data. Data are labeled when the correct outcome is known and the outcome data are present during training. For example, if you wanted to train an algorithm to discriminate between malignant and benign tumors, you would need training data which include all data points needed to make that determination (e.g., mass size, shape, location, etc.) and an additional data point with the correct outcome.  \n",
    "2. Unsupervised Learning: Unsupervised Learning refers to model training that has no predetermined outcome but is instead seeking to organize the inputs in a discriminatory way. For example, a retailer may not know what types of customers shop at their store, but they could use unsupervised learning to ‘find’ different types of customers based on shopping histories.  \n",
    "3. Semi-Supervised Learning: Semi-supervised learning refers to model training that uses partially labeled data. In some situations, labelling all training data is cost-prohibitive or impossible (e.g., finding radiologists who are willing to label images of tumors is expensive). In this case, supervised and unsupervised technique are combined to predict the desired outcome. Unsupervised learning is used to find the desired outcome in the unlabeled data and its results are iteratively verified by the labeled data.  \n",
    "\n",
    "Learning algorithms are said to ‘converge’ on the correct solution. This happens iteratively as the algorithm seeks to reduce error (i.e. the difference between the value predicted by the algorithm and the actual output) as determined by a loss function. Each iteration (or Epoch), the algorithm adjusts the weights—the influence each input parameter has on predicting the outcome—until the error is minimized.  \n",
    "\n",
    "## Machine Learning Process\n",
    "Machine learning exercises tend to follow a series of steps that are common to all projects.  \n",
    "\n",
    "1. Problem Identification: Machine learning algorithms are designed to find relationships between inputs and outputs, regardless of whether it is appropriate to do so. Therefore, any machine learning task must begin with an appropriate understanding of what you are trying to achieve.  \n",
    "2. Data Collection: Beginning with problem identification is important because it will also identify possible data sources and data elements that are needed to solve the problem. Once identified, you must set out to collect the data needed to address the focal problem.  \n",
    "3. Data Profiling: Real-world is often messy, inconsistent, and incomplete. Thus, all projects must undergo a process of profiling and processing the data so that they are appropriate for modeling. Data profiling often involves a variety of tasks including exploring data, scaling variables, addressing missing values, and splitting the dataset into training and testing data.  \n",
    "4. Model Specification: Once cleaned, the data are ready to be used for training a model. At this point, the analyst must choose an algorithm that is appropriate given the inputs and desired outputs and then define the hyperparameters, learning parameters of the chosen algorithm, and convergence criteria.  \n",
    "5. Model Evaluation: A converging model is not necessarily a good solution. To assess the quality of the model, test data are used to evaluate model predictions. Alternatively, models may be compared to alternative algorithms to assess quality.\n",
    "6. Conclusion: Model building is not (always) an educational exercise. Often, we build machine learning models to predict outcomes. So, how did this model do? Where is it right and where is it wrong? Is it more important to be right than it is to be not wrong? Are the costs of being wrong so high that we have little to no tolerance for error?\n",
    "\n",
    "## Classification\n",
    "Classification is a fundamental task in machine learning where the objective is to predict the categorical label of new observations based on past observations. This type of task is essential in scenarios ranging from email spam detection to medical diagnoses. The process involves training a model on a labeled dataset where the correct label is known. The model then learns to map the input features to the desired output labels by minimizing the discrepancy between its predictions and the actual labels. Various algorithms, such as logistic regression, support vector machines, and neural networks, are employed to perform classification tasks. These models can handle binary classification (two classes) or multi-class classification (multiple classes), depending on the problem at hand.  \n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"https://thislondonhouse.com/Jupyter/Images/svm.jpg\"></p>\n",
    "\n",
    "Once trained, the classifier is evaluated using performance metrics such as accuracy, precision, recall, and F1-score to ensure its effectiveness. An important aspect of classification is handling class imbalances, which occur when some classes are underrepresented in the training data. Techniques such as oversampling, undersampling, or employing specialized loss functions help address this challenge. With advancements in machine learning, classifiers can now handle complex and high-dimensional datasets, making them instrumental in various fields including finance, healthcare, and social media analytics. Continual improvements in algorithms and computational power are constantly pushing the boundaries of what is possible with classification models.  \n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"https://thislondonhouse.com/Jupyter/Images/confusionMatrxiUpdated.jpg\"></p>\n",
    "\n",
    "### Classification Algorithms\n",
    "#### Logistic Regression\n",
    "Logistic regression is a simple yet powerful classification technique used for binary and multi-class classification problems. Despite its name, logistic regression is actually a linear model for classification rather than regression. It predicts the probability of a binary outcome using a logistic function, also known as the sigmoid function. The output of the logistic regression model is a value between 0 and 1, which represents the probability of the positive class. The model estimates the coefficients of the input features by minimizing the logistic loss function, which measures the difference between the predicted probabilities and the actual labels. Logistic regression is widely used due to its simplicity, interpretability, and effectiveness on linearly separable data. However, it may not perform well on complex datasets where the relationship between the input features and the output is highly non-linear.\n",
    "#### Support Vector Machines\n",
    "Support Vector Machines (SVM) are powerful classification algorithms that aim to find the optimal hyperplane that separates the data into different classes. The SVM algorithm tries to maximize the margin between the classes, which is the distance between the hyperplane and the nearest data points from each class, known as support vectors. By doing so, SVM achieves a robust and generalizable decision boundary. For non-linearly separable data, SVM uses a technique called the kernel trick to transform the input features into a higher-dimensional space where a linear separation is possible. Common kernel functions include polynomial, radial basis function (RBF), and Gaussian. SVMs are particularly effective in high-dimensional spaces and have been widely used for tasks such as image recognition and bioinformatics. However, SVMs can be computationally expensive and less efficient on very large datasets.\n",
    "#### Naive Bayes Classifier\n",
    "Naive Bayes classifiers are probabilistic models based on Bayes' Theorem, which leverages the concept of conditional probability. The \"naive\" aspect of the algorithm refers to the assumption that the features are independent given the class label. Despite this strong assumption, Naive Bayes classifiers have been highly successful in various applications, particularly in text classification tasks such as spam detection and sentiment analysis. The model calculates the probability of each class given the input features and selects the class with the highest probability as the predicted label. There are different types of Naive Bayes classifiers, including Gaussian, Multinomial, and Bernoulli, each suited for different types of data. One major advantage is their simplicity and efficiency; however, their performance can be suboptimal if the feature independence assumption is violated.\n",
    "#### Neural Network Classifiers\n",
    "Neural network classifiers are inspired by the human brain and consist of multiple layers of interconnected artificial neurons. These models are capable of learning complex, non-linear decision boundaries and are highly flexible, making them suitable for a wide range of applications. A neural network consists of an input layer, one or more hidden layers, and an output layer. Each neuron in a layer performs a weighted sum of its inputs, applies an activation function, and passes the result to the next layer. During training, the model adjusts the weights using a process called backpropagation, which minimizes the error between the predicted and actual labels. With the advent of deep learning, neural networks with many hidden layers, known as deep neural networks, have achieved remarkable performance in tasks such as image classification, speech recognition, and natural language processing. These models excel at discovering intricate patterns in large and complex datasets. However, they require substantial computational resources and large amounts of data, and they can be more challenging to interpret compared to simpler models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35667294-83d7-4bc7-b3bc-febc26ad8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c5945d-1377-4f45-be83-eabbbe956ee6",
   "metadata": {},
   "source": [
    "## Classification Exercise 1\n",
    "### Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbab606-eab2-4849-b012-b30c85a97fd3",
   "metadata": {},
   "source": [
    "It is important for blood banks to be able to correctly identify individuals who are likely to donate blood at a blood drive. Given that blood banks often have very limited resources for incentivizing donations, it would be wise to reserve incentives for those who are more likely to need encouragement (i.e., those who are less likely to donate at any given blood drive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87cdf0c-2a2d-4f75-b48e-065d33b482e0",
   "metadata": {},
   "source": [
    "### Data Collection/Selection\n",
    "We will be loading data from a blood donation dataset. More information is available [here](https://archive.ics.uci.edu/dataset/176/blood+transfusion+service+center).  \n",
    "\n",
    "Data are orgnized in tabular format with each record representing an individual donor. The target variable is 'DonatedBlood' which represents whether the individual donated at the most recent blood drive. The data contain the following columns:  \n",
    "| Variable Name | Role | Type | Description |\n",
    "| ------------- | ---- | ---- | ----------- |\n",
    "| Recency | Feature | Integer | months since last donation |\n",
    "| Frequency | Feature| Integer | total number of donations |\n",
    "| DonatedAmount | Feature | Integer|total blood donated in c.c. |\n",
    "| Time | Feature | Integer | months since first donation |\n",
    "| DonatedBlood | Target | Binary | whether he/she donated blood in March 2007 (1 stand for donating blood; 0 stands for not donating blood) |\n",
    "\n",
    "The following line will load the data as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cab0ca-29db-41a5-932e-9c739b7c1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "blood_donation_df = pd.read_csv(\"data/blood_donation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a9a7b3-e9f9-4c4b-9b8d-7787358e36f8",
   "metadata": {},
   "source": [
    "### Data Profiling  \n",
    "Once the data are loaded, we need to profile the data and prepare it for analysis. This typically involves several steps that may include handling missing data, exploring data, feature selection, among others. The steps will vary depending on the dataset and the business problem, but profiling always precedes model building.  \n",
    "\n",
    "The following lines provide important insight into the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e920e6-02dc-4bbd-9a53-ed6d1917ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blood_donation_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf4982-a49c-4ad4-ac9a-a9ae5bce592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blood_donation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed45962-5b16-4c08-b302-ea9c3407b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "blood_donation_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d2f4e-c564-46e8-85f6-44ae558a8f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, values in blood_donation_df.items():\n",
    "    if values.dtypes in ['int64', 'float64']:\n",
    "        values.hist()\n",
    "    else:\n",
    "        values.value_counts().plot.barh()\n",
    "    plt.title(f'{feature}') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa7727-5846-4d21-91c3-94c13c211cec",
   "metadata": {},
   "source": [
    "The following lines will define which features (i.e., columns) we want to include in our model and groups them based on the data type included in each column. We may change our mind, but these steps will serve as a starting point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64e384-1629-46a2-bb1b-4390c6efe91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = []\n",
    "numeric_cols = []\n",
    "count_cols = ['Recency', 'Frequency', 'DonatedAmt', 'Time']\n",
    "target_cols = ['DonatedBlood']\n",
    "input_cols = [x for x in categorical_cols + numeric_cols + count_cols if x not in target_cols]\n",
    "data_cols = input_cols + target_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef032de-4894-48a4-9478-a6fc2ca7986f",
   "metadata": {},
   "source": [
    "Once we've decided on which features to include, it is sometimes easiest to subset these features into a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43736d7c-e1e4-4180-803b-ce00e660de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = blood_donation_df[data_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f98d24-07b8-43cd-865a-0435fc4ec6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fb0fd-0aae-44ac-a6d8-85e0953b0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb966869-ca88-40a6-a687-23e9a9798bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target_cols].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc272b6-44f3-4a1e-851d-624d6b6c324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in target_cols:\n",
    "    df[target].value_counts().plot.barh()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e6aa8-0c0f-43a2-ba6b-c07e986073d5",
   "metadata": {},
   "source": [
    "We can view a pair plot of each feature separated by the target variable. This can be helpful for spotting any problems in our data or relationships among features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f03725a-3a8a-499d-9b6c-e2fe0f58ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in target_cols:\n",
    "    sns.pairplot(df, hue=target, corner=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea3268-3d6b-4546-be66-32b263213bc3",
   "metadata": {},
   "source": [
    "Once we are happy with our dataset, we can drop records with missing data, split the data into testing and training sets, and move on to model specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58f0c2-345b-4a11-b381-6e735f0e4868",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3635ba9-4fd2-4934-9619-1010c15d4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[input_cols], df[target_cols], test_size=0.25, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146cc40a-f012-4bb5-8946-eae9bde73e3e",
   "metadata": {},
   "source": [
    "### Model Specification\n",
    "Specifying a model usually requires you to make decisions about whether/how to deal with data transformations and selecting an appropriate model. We will be using a concept known as pipelines to address data transformations and model selection. Pipelines make our lives easier by collecting a series of common steps into a single pipeline. This pipeline of steps will execute whenever the model is run, ensuring consistency and reducing the likelihood of error.  \n",
    "\n",
    "Because different types of data are transformed in different ways, we will build three different transformer pipelines, one for numbers, one for count values, and one for categorical values. Each transformer will only be applied to the columns that contain the specifed data type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7df765-afc8-4392-b56d-1210d7c6d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_transformer = Pipeline(steps=[\n",
    "    ('log', FunctionTransformer(np.log1p, feature_names_out='one-to-one'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf7a444-4f54-486e-a0be-6bd7808e48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be40ef5d-de81-4655-9365-3072344bdc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ad82e-0a35-4cb2-98d2-84f197a88484",
   "metadata": {},
   "source": [
    "Each transformer is an individual transformer, but we are going to collect each into a preprocessor pipeline that is specific to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07152b85-a14d-4a61-af51-a156869cdb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('count', count_transformer, count_cols),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41b24e-6163-4bd0-82f2-68ad9ac5958e",
   "metadata": {},
   "source": [
    "#### Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331b996-e8ff-41e4-bf16-b3c8f9819c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('logistic', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4608f673-18eb-40d4-a860-686b7268e62c",
   "metadata": {},
   "source": [
    "With the pipeline built, we are ready to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c249242-2590-4811-84ec-2d1a0f529fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipeline.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61574e52-f7c7-48d3-b65d-8fb54c51b756",
   "metadata": {},
   "source": [
    "To assess the performance of the model, we will use the data we reserved for test to predict the target variable (donate/not dontate). This will help us benchmark the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b9284f-0bc2-4287-82b7-6c174b4493d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_predicted = logistic_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a5fa2-46d3-44a3-8db5-2b3fa8763cda",
   "metadata": {},
   "source": [
    "If we look at the results side-by-side, we can see the logistic regression model did a pretty good, but not perfect, job. But, how good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed8d965-4cec-4f72-81f5-c9fef237b865",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame({f\"{target_cols[0]}-True\": np.ravel(y_test), \n",
    "                    f\"{target_cols[0]}-Predicted\": logistic_predicted\n",
    "                   }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529be38-cc6c-4b88-8a1d-1c19e7496c41",
   "metadata": {},
   "source": [
    "Here we can see whether the test value matchest the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8d8f3-6ea3-490e-a541-4c3aa4f53d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.ravel(y_test) == logistic_predicted, columns=['Correctly Predicted']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c45665-3a3f-44c2-988f-78c90a2bf711",
   "metadata": {},
   "source": [
    "And if we take the average of these results (Note, True is treated as a 1 and False is treated as a 0, so the average tells us the percentage of True's), we can see the **Accuracy** of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deae6b9-782f-4ca7-bddd-275074087c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.ravel(y_test) == logistic_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d41269-c6bf-4cfe-b66d-a50170581e31",
   "metadata": {},
   "source": [
    "This is a common task so there are built-in tools for assessing classifer performance. The following function summarizes the most important metrics for classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e4562-32e0-4af0-81a1-be5e1fd398c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_performance(y, y_pred, labels_dict=None):\n",
    "    accuracy = metrics.accuracy_score(y, y_pred)\n",
    "    precision = metrics.precision_score(y, y_pred, average='weighted')\n",
    "    recall = metrics.recall_score(y, y_pred, average='weighted')\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y, y_pred)\n",
    "    f1 = metrics.f1_score(y, y_pred, average='weighted')\n",
    "    report = metrics.classification_report(y, y_pred, target_names=[labels_dict[i] for i in sorted(labels_dict.keys())])\n",
    "\n",
    "    # Display the confusion matrix with custom labels\n",
    "    conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[labels_dict[i] for i in sorted(labels_dict.keys())])\n",
    "    disp.plot(cmap=plt.cm.Greens)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(report)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f47e2f-c41c-4257-a1ec-78b2ff2d7822",
   "metadata": {},
   "source": [
    "If we pass in the results of our current model to the function above, we will get the performance of this model. Then, we could run another model and reuse the function to have a common platform for assessing classifer perforamnce across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cefe7b4-68ef-4dfa-93be-69a2861a24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Classification Performance Metrics:\")\n",
    "classifier_performance(y_test, logistic_predicted, {0: 'donated', 1: 'not donated'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0050c0-26be-4915-a4b0-aa3d98f90895",
   "metadata": {},
   "source": [
    "**For reference**  \n",
    "Accuracy = (TN + TP)/(TN + TP + FN + FP)  \n",
    "Precision = (TP)/(TP + FP)  \n",
    "Sensitivity (Recall) = (TP)/(TP + FN)  \n",
    "Specificity (Selectivity) = (TN)/(TN + FP)  \n",
    "Balanced Accuracy = (Sensitivity + Specificity)/2  \n",
    "F1 Score = (2 × Precision × Recall)/(Precision + Recall)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb08f7-0496-43ee-a3e8-7b5976e9c402",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27f82d-5985-4a11-bfe2-569bb6a3566f",
   "metadata": {},
   "source": [
    "Is the logistic model any good? Can we do better? The only way to know is to run more models. To get a baseline, we can run a model that ignores all inpts and simply predicts the most common outcome, 'not donated' in this case. This type of model should be the worst we could do.  \n",
    "\n",
    "Since we already have a pipeline set up, we can reuse most of our work and change the final model.\n",
    "#### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d77590c-b2a4-4363-8315-db4ee6a54174",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dummy', DummyClassifier(strategy=\"most_frequent\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b54d6-a1bd-4885-8c89-1b376c953fb4",
   "metadata": {},
   "source": [
    "Next, we will fit the model, as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f300775-d46e-4625-ae1d-e9dd6e2e46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22db52-f19d-47e4-b8de-2627092ad3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_predicted = dummy_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6fb8a-99e8-44f3-aa16-fe2b760e78b6",
   "metadata": {},
   "source": [
    "Now, we can reuse our classifier performance function to get a feel for how the dummy classifier performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f96606-5059-495c-9316-fb1064a2403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dummy Classification Performance Metrics:\")\n",
    "classifier_performance(y_test, dummy_predicted, {0: 'donated', 1: 'not donated'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f82b53-0419-4a32-a0b1-00c9c53ba7d6",
   "metadata": {},
   "source": [
    "These results show a reasonable level of accuracy (76%, compared to 79% for the logistic regression), but precision and balanced accuracy scores are much lower (because it wasn't even trying to predict). This illustrates one of the challenges of highly unblanced datasets: models seeking correct predictions often struggle to outperform the most common result and tend to revert to the most frequent response.\n",
    "\n",
    "Let's try another model!\n",
    "#### Support Vector Machines\n",
    "Again, we will reuse the pipeline we've created and just change the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453df52a-b593-458c-9271-e90dbd47842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('svm', LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28df233-d0d6-4f53-aa30-226261e6f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipeline.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4569ee-d3e9-463c-9aaf-726b7206089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predicted = svm_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb7cc8c-ff3e-4ef7-8dfd-7f58bd209006",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM Classification Performance Metrics:\")\n",
    "classifier_performance(y_test, svm_predicted, {0: 'donated', 1: 'not donated'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdca3d-7e3e-423e-afa2-61f489c9429f",
   "metadata": {},
   "source": [
    "A little bit better than guessing, but not much. Logistic is still the model to beat.\n",
    "\n",
    "So, let's try another!\n",
    "#### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067222f1-0847-4027-a66c-1c59a9ef2c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('nb', GaussianNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b8afa-8abb-4cc2-81c3-ced31d36e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipeline.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b46875-394c-45cd-97e2-79292f2c5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predicted = nb_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8934d-63ee-45f9-b0f1-48a7a8290bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_performance(y_test, nb_predicted, {0: 'donated', 1: 'not donated'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112b708-0707-47c2-af2f-8dadb8d1a725",
   "metadata": {},
   "source": [
    "The best overall model, thus far. Let's do one more!  \n",
    "#### Neural Network Classifier  \n",
    "Neural Networks work a little differently as they have no a priori structure. Instead, the analyst builds the model and the model then tries to detect patterns predicting the target via the defined network. As such, there are many more choices that can be made than are defined here. We will look at these models more as the semester progresses and what it takes to tune a neural network. For now, we will use the model defined below and we will insert it into our exisiting pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9886151-9eba-4341-a01d-aba812da39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequential_model(dims, metric):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(dims,)))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994408c-ac92-496a-ad3c-c5bda21d6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('nn', KerasClassifier(model=create_sequential_model(preprocessor.fit_transform(X_train).shape[1], ['accuracy']), epochs=10, batch_size=5, verbose=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c68a3e-23d0-46c2-bf2d-4269953a0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf5b1d-43d3-4ddb-b50b-9b60ee7a1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_predicted = nn_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0e7f1-4f1d-47b8-9f28-a0e220fb1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Neural Network Classification Performance Metrics:\")\n",
    "classifier_performance(y_test, nn_predicted, {0: 'donated', 1: 'not donated'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe9ecb-bea8-4ced-b332-3bb272d31eac",
   "metadata": {},
   "source": [
    "Not great. It is just guessing 'not donated' and therefore no smarter than the dummy classifier. But, neural networks have optimization objectives and we will get different results if we ask the model to prioritize different objectives. For the initial model, we prioritized **accuracy**. What happens if we prioritize **precision**?  \n",
    "#### Feature Selection\n",
    "Just because we have data doesn't mean we will want to use it. Some values may be unrelated to our target. Others may confound and lead to irrational decisions. Also, for complex models and large datasets, more data may adversely affect the performance of the model--a slow but accurate model may be worse than a fast but kinda accurate model. So, it is important that we consider the role that features have in predicting the target. Some will have a large role and some will have a small role. Those with small roles may **sometimes** be excluded without harming the overall performance of the model. You will have to use trial and error to make the decisions. *Note: Coefficients are not available for all models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1aa87-beb8-4a01-9e6e-188089a3f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model_pipeline, preprocessor_name, model_name):\n",
    "    all_feature_names = []\n",
    "    [all_feature_names.extend(transformer.get_feature_names_out()) for key, transformer in model_pipeline.named_steps[preprocessor_name].named_transformers_.items()]\n",
    "    pd.DataFrame(\n",
    "        model_pipeline[model_name].coef_[0] * np.std(logistic_pipeline[preprocessor_name].fit_transform(X_train), axis=0),\n",
    "        columns=[\"Coefficients\"],\n",
    "        index=all_feature_names\n",
    "    ).plot(kind=\"barh\", figsize=(7, 8))\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179cdb53-42b1-4678-91d0-46ff755a1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(logistic_pipeline, 'preprocessor', 'logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf55f55e-6ec2-4004-b076-d577861191dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(svm_pipeline, 'preprocessor', 'svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783e12d-9ec8-4333-8325-e773bc76a89d",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Is this a good model? That depends. What is the risk of misclassifying? If you predict someone as a donor, but they do not donate, what are the consequences? If you predict someone will not donate, but they do, is that bad? Assessing the quality of the model involves not only looking at predictive quality but also considering the cost of misclassification.  \n",
    "\n",
    "In this case, the models are fine but not great. The best model is moderately better than just assuming someone will not donate, which would be considered a guess in most cases, but given the unbalanced nature of this dataset, is a highly likely outcome. Given the moderate prediction performance, the more expensive (with regard to time/resources) algorithms are less preferred than a standard logistic. Fortunately, the costs of misclassifying are relatively low as it would likely mean that you end up devoting resources to attract a donor who would have donated without the incentives. This is illustrated in the confusion matrix where we see that because the models are biased toward the negative, all algorithms place a large number of positives (donate) in the negative (not donate) column (false negatives). Fortunately, the risks/costs associated with false positives are low for blood drives.  \n",
    "\n",
    "Unfortunately, the best course of action would be to improve the dataset (which is often expensive or impossible). We only have four variables and two of them are perfectly correlated (which means we really only have three input variables). It would be nice to have some more variables about the donors, the donation event, and the event location as it seems likely that all three could have an impact on whether or not someone donates blood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d0d6b-deaf-46a2-938e-e45156385e7c",
   "metadata": {},
   "source": [
    "## Classification Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230bfda3-69d1-45ca-a33c-99905def8f8b",
   "metadata": {},
   "source": [
    "### Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16becf8e-9e3b-425b-afe5-fd24d2b26059",
   "metadata": {},
   "source": [
    "Offering credit is risky. Banks need to offer credit to a variety of customers because low-risk customers are of little value (because they pay off their balance monthly or they maintain very small balances) but high-risk customers may have negative value if they are prone to default on their debts. So, it is important for banks to be able to accurately identify customers who are likely to default on their credit balances so as to limit their exposure to potential loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48785c5-4303-4c57-9ea8-4b4f1175d957",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc126ee-9b14-443a-8eec-4ff50d723043",
   "metadata": {},
   "source": [
    "We will be loading data from a credit card clients dataset. More information is available [here](https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients).  \n",
    "\n",
    "Data are orgnized in tabular format with each record representing an individual customer. The target variable is 'Default' which represents whether the individual donated at the most recent blood drive. The data contain the following columns:  \n",
    "| Variable Name | Role | Type | Description |\n",
    "| ------------- | ---- | ---- | ----------- |\n",
    "| LIMIT_BAL | Feature | Integer | Current credit borrowing limit |\n",
    "| SEX | Feature | | |\n",
    "| EDUCATION | Feature | | |\n",
    "| MARRIAGE | Feature | | |\n",
    "| AGE | Feature | | |\n",
    "| PAY_0 | Feature | | |\n",
    "| PAY_2 | Feature | | |\n",
    "| PAY_3 | Feature | | |\n",
    "| PAY_4 | Feature | | |\n",
    "| PAY_5 | Feature | | |\n",
    "| PAY_6 | Feature | | |\n",
    "| BILL_AMT1 | Feature | | |\n",
    "| BILL_AMT2 | Feature | | |\n",
    "| BILL_AMT3 | Feature | | |\n",
    "| BILL_AMT4 | Feature | | |\n",
    "| BILL_AMT5 | Feature | | |\n",
    "| BILL_AMT6 | Feature | | |\n",
    "| PAY_AMT1 | Feature | | |\n",
    "| PAY_AMT2 | Feature | | |\n",
    "| PAY_AMT3 | Feature | | |\n",
    "| PAY_AMT4 | Feature | | |\n",
    "| PAY_AMT5 | Feature | | |\n",
    "| PAY_AMT6 | Feature | | |\n",
    "| DEFAULT | Target| |  |\n",
    "\n",
    "The following line will load the data as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5978345-2217-4bb1-817e-fd0d4ea6a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_default_df = pd.read_csv(\"data/credit_card_default.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45073f48-2741-4633-8801-55194953d3ee",
   "metadata": {},
   "source": [
    "### Data Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0a54a-ca0a-4856-aeee-523a2a1e1d31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d4a25c-83f0-4f85-9952-3ea55c003e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(credit_default_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc3404-6433-48b8-9993-0e7574eb5a12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de53e04-2aae-4de6-bfb3-16144b635172",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(credit_default_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d694a3-1d2f-4215-bf1a-678556ea27d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be200da9-181a-4810-90e5-578bd3d6bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(credit_default_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3eac1b-496a-4ee4-baa8-976ff4eff7f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeeb453-960b-4aa6-9665-303d0c840cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, values in credit_default_df.items():\n",
    "    if values.dtypes in ['int64', 'float64']:\n",
    "        values.hist()\n",
    "    else:\n",
    "        values.value_counts().plot.barh()\n",
    "    plt.title(f'{feature}') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03426464-91ef-4978-82da-963b98a6ff69",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27779d70-ea12-484d-a64f-e808a6371e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "numeric_cols = ['LIMIT_BAL', 'BILL_AMT1', 'PAY_AMT1', 'BILL_AMT2', 'PAY_AMT2', 'BILL_AMT3', 'PAY_AMT3', 'BILL_AMT4', 'PAY_AMT4', 'BILL_AMT5', 'PAY_AMT5', 'BILL_AMT6', 'PAY_AMT6']\n",
    "count_cols = []\n",
    "target_cols = ['DEFAULT']\n",
    "input_cols = [x for x in categorical_cols + numeric_cols + count_cols if x not in target_cols]\n",
    "data_cols = input_cols + target_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aeb31d-7a42-42ee-a6c7-d4735d66cc2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d8e25-b066-45ab-80e0-2545bbb73c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = credit_default_df[data_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4cef55-8b7a-4783-89b0-21068199fb61",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b063e5-3fd0-496e-9d6d-fc838bf4eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912e0d2-8c2b-45b8-b28b-88e0eefb4eaf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b595f7-882f-444b-8324-420f9021e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in target_cols:\n",
    "    sns.pairplot(df, hue=target, corner=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82875baa-22cb-43b2-bf8a-d8564916ff87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba8dab-eacc-4480-9321-48ce21ffe44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85952643-fdb4-48a4-b0f7-9813ada9d2fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886115b3-007a-4883-b83c-0691e20167f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[input_cols], df[target_cols], test_size=0.25, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab8ad4-5bba-402d-b5ed-46f63d3ae309",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c9b18-605d-4307-a82a-a299ad1a6495",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775932f-a1c6-4ade-b098-5e7811851c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', categorical_transformer, categorical_cols),\n",
    "        ('numeric', numeric_transformer, numeric_cols),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e75bb3-c6eb-49e2-8f75-b33c807dcb85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99266243-8994-4c07-8e92-0ea1831f3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Model\n",
    "dummy_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dummy', DummyClassifier(strategy=\"most_frequent\"))\n",
    "])\n",
    "dummy_pipeline.fit(X_train, y_train)\n",
    "dummy_predicted = dummy_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0558a8-2f51-4b47-970b-816da896c81f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e9440-de59-433f-8045-85127b72d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Model\n",
    "logistic_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('logistic', LogisticRegression())\n",
    "])\n",
    "logistic_pipeline.fit(X_train, np.ravel(y_train))\n",
    "logistic_predicted = logistic_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc67e6-cec1-4760-a7c8-eaf71d1dae10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2652bf0d-b32b-4531-a388-7a353cbbd241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "nb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('nb', GaussianNB())\n",
    "])\n",
    "nb_pipeline.fit(X_train, np.ravel(y_train))\n",
    "nb_predicted = nb_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb343eb-a654-45b0-9a96-189f7602c9a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b682de3-4308-4cf9-ba9e-782fae778793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model\n",
    "svm_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('svm', LinearSVC())\n",
    "])\n",
    "svm_pipeline.fit(X_train, np.ravel(y_train))\n",
    "svm_predicted = svm_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a8de2-bafe-47df-a654-13af9836a6c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59949a-0b05-4c97-a79d-27e54d1ddd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "nn_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('nn', KerasClassifier(model=create_sequential_model(preprocessor.fit_transform(X_train).shape[1], ['accuracy']), epochs=10, batch_size=5, verbose=1))\n",
    "])\n",
    "nn_pipeline.fit(X_train, y_train)\n",
    "nn_predicted = nn_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c362b-90f5-45a2-abb2-fc8033b3a7f0",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15dd78e-a401-44bd-90c2-95482166e18b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ac089-b007-4b8a-a7a1-c9eee27b1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dummy Classification Performance Metrics:\")\n",
    "classifier_performance(y_test, dummy_predicted, {0: 'donated', 1: 'not donated'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d4364-9943-40c8-87d9-5832aa7be60f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acda6cf-bb4a-4d21-9812-4ee3dd70b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Classification Performance Metrics:\")\n",
    "classifier_performance(y_test, logistic_predicted, {0: 'default', 1: 'not default'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a5513-4ced-4ec0-b814-b2c75664e1b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb82cf-95ed-433a-8779-da87757ba6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive Bayes Classification Performance Metrics:\")\n",
    "classifier_performance(y_test, nb_predicted, {0: 'donated', 1: 'not donated'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f110aa-cb44-41c7-96f8-9da65ef0722a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31a76f-6cbe-4e83-8b63-568fc4cd36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM Classification Performance Metrics:\")\n",
    "classifier_performance(y_test, svm_predicted, {0: 'default', 1: 'not default'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5b637-edfb-48ab-a528-ba4eb2ccf1dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08727ceb-426d-45b3-a4bd-b92b165f64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Neural Network Classification Performance Metrics:\")\n",
    "classifier_performance(y_test, nn_predicted, {0: 'default', 1: 'not default'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7eae5f-eb1d-47a6-91f5-36f7fb57bd39",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848aa1a-2182-4e5b-95f2-227c284e65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(logistic_pipeline, 'preprocessor', 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2537f-6f7d-402b-87ee-9ddb204988f3",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645d7aa7-c024-4064-9634-a38849a5e379",
   "metadata": {},
   "source": [
    "Is this a good model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db08de-3c32-4f54-ad6e-ee4e6a48caa6",
   "metadata": {},
   "source": [
    "## AI as Person\n",
    "\n",
    "AI is terrible at acting like traditional software, but it is increasingly capable of acting like a person. What implications will that have for us, for the marketplace, and for society?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
